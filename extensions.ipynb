{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 141513,
     "status": "ok",
     "timestamp": 1748807399973,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "94Z1Mc-Cb7gx",
    "outputId": "4a21311b-4560-449d-c5c4-74be0906a869"
   },
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
    "\n",
    "# Downgrade numpy to a compatible version\n",
    "!pip install numpy==1.23.5 --force-reinstall\n",
    "!pip install nltk"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1748807434390,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "-oBE-NTU88VF",
    "outputId": "58cd1683-e87d-44ea-e3ed-d59afcf5d286"
   },
   "source": [
    "!git clone https://github.com/ezmi234/Affordance_Highlighting_Project_2024.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1748807434409,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "elZUKZwE8-5E",
    "outputId": "9d436b59-7fe7-4f00-d5ca-a4b2c9a640a8"
   },
   "source": [
    "%cd Affordance_Highlighting_Project_2024\n",
    "!git checkout extensions-experiments"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2030,
     "status": "ok",
     "timestamp": 1748807436617,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "IXU6wCaDWR0K",
    "outputId": "e34fb52e-60ba-4a5b-dfae-08341a568d18"
   },
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 81169,
     "status": "ok",
     "timestamp": 1748807517840,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "4ozrcjricJCs",
    "outputId": "2bffa5c8-257b-40ea-eb8c-b714bf19ae55"
   },
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import time\n",
    "from utilities.prompt_enricher import generate_prompts\n",
    "from utilities.positional_encoding import PositionalEncoding, FourierFeatureTransform, LocalPositionalEncoding, NGPHashEncoding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1748807518156,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "EehETcVLZG1D"
   },
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3,\n",
    "                 hidden_dim=256,\n",
    "                 output_dim=2,\n",
    "                 num_layers=6,\n",
    "                 positional_encoding=False,\n",
    "                 sigma=5.0,\n",
    "                 encoding_type='none',\n",
    "                 num_frequencies=10,\n",
    "                 grid_resolution=16,\n",
    "                 hash_levels=16,\n",
    "                 hash_features_per_level=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Select the appropriate encoding\n",
    "        if encoding_type == 'none':\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "\n",
    "        elif encoding_type == 'positional':\n",
    "            layers.append(PositionalEncoding(input_dim, num_frequencies))\n",
    "            layers.append(nn.Linear(input_dim + 2 * input_dim * num_frequencies, hidden_dim))\n",
    "\n",
    "        elif encoding_type == 'fourier':\n",
    "            layers.append(FourierFeatureTransform(input_dim, hidden_dim, sigma))\n",
    "            layers.append(nn.Linear(hidden_dim * 2 + input_dim, hidden_dim))\n",
    "\n",
    "        elif encoding_type == 'local':\n",
    "            layers.append(LocalPositionalEncoding(grid_resolution, num_frequencies))\n",
    "            layers.append(nn.Linear(2 * num_frequencies * 3, hidden_dim))\n",
    "\n",
    "        elif encoding_type == 'hash':\n",
    "            layers.append(NGPHashEncoding(\n",
    "                input_dim=input_dim,\n",
    "                n_levels=hash_levels,\n",
    "                n_features_per_level=hash_features_per_level,\n",
    "                log2_hashmap_size=19\n",
    "            ))\n",
    "            total_features = hash_levels * hash_features_per_level\n",
    "            layers.append(nn.Linear(total_features, hidden_dim))\n",
    "\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.LayerNorm([hidden_dim]))\n",
    "\n",
    "        # Append hidden layers\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm([hidden_dim]))\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1748807518202,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "ih_V7jBzYzCJ"
   },
   "source": [
    "def clip_loss(rendered_images, text_prompts, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0, weights=None):\n",
    "    loss = 0.0\n",
    "\n",
    "    # Encode text\n",
    "    text_tokens = tokenizer(text_prompts).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
    "\n",
    "    num_prompts = len(text_prompts)\n",
    "    if weights is None:\n",
    "        weights = [1.0 / num_prompts] * num_prompts\n",
    "    else:\n",
    "        total = sum(weights)\n",
    "        weights = [w / total for w in weights]\n",
    "    weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        image_features = clip_model.encode_image(clip_image).float()\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        weighted_sum = 0.0\n",
    "        for i in range(num_prompts):\n",
    "            similarity = torch.cosine_similarity(image_features, text_features[i].unsqueeze(0)).mean()\n",
    "            weighted_sum += weights[i] * similarity\n",
    "\n",
    "        loss -= weighted_sum\n",
    "    else:\n",
    "        for _ in range(n_augs):\n",
    "          aug_image = aug_transform(rendered_images)\n",
    "          image_encoded = clip_model.encode_image(aug_image)\n",
    "\n",
    "          weighted_sum = 0.0\n",
    "          for i in range(num_prompts):\n",
    "              similarity = torch.cosine_similarity(image_encoded, text_features[i].unsqueeze(0)).mean()\n",
    "              weighted_sum += weights[i] * similarity\n",
    "\n",
    "        loss -=  weighted_sum / n_augs\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1748807518569,
     "user": {
      "displayName": "Ezmiron Deniku",
      "userId": "06212827352679775598"
     },
     "user_tz": -120
    },
    "id": "Mq2VLkE_YCs9"
   },
   "source": [
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==== Hyperparameters and Settings ====\n",
    "render_res = 224\n",
    "learning_rate = 0.00005\n",
    "n_iter = 2200\n",
    "obj_path = 'data/Auto.obj'\n",
    "n_augs = 3\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-B/32'\n",
    "prompt = 'A gray car with highlighted wheels'\n",
    "affordance = 'wheels'\n",
    "semantic_class = 'car'\n",
    "prompt_template = 'A gray {} with highlighted {}'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== Normalization and Augmentation ====\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((render_res, render_res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# ==== Load Mesh ====\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "\n",
    "# ==== Background Image ====\n",
    "# background_img_path = 'images/background.jpg'\n",
    "background_img_path = None\n",
    "img_bg = None\n",
    "bg_tensor = None\n",
    "\n",
    "def load_image(image_path, size=None):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    if size is not None:\n",
    "        image = image.resize(size, Image.BILINEAR)\n",
    "    return image\n",
    "\n",
    "if background_img_path is not None:\n",
    "    img_bg = load_image(background_img_path, size=(render_res, render_res))\n",
    "    bg_tensor = transforms.ToTensor()(img_bg).unsqueeze(0).to(device)\n",
    "\n",
    "# ==== Setup Output Directory ====\n",
    "\n",
    "base_path = '/content/drive/MyDrive/extensions'\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "synonyms_exts = True\n",
    "\n",
    "for pe in [\n",
    "    'none',\n",
    "    # 'positional',\n",
    "    # 'fourier', 'local',\n",
    "    # 'hash'\n",
    "    ]:\n",
    "    print(\"Positional encoding:\", pe)\n",
    "    print(\"Synonyms:\", synonyms_exts)\n",
    "\n",
    "    # Constrain most sources of randomness\n",
    "    # (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "    # ==== Set Seed for Determinism ====\n",
    "    seed = 420\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # ==== Neural Highlighter ====\n",
    "    mlp = None\n",
    "\n",
    "    if pe == 'none':\n",
    "        mlp = NeuralHighlighter(encoding_type='none')\n",
    "    elif pe == 'positional':\n",
    "        mlp = NeuralHighlighter(encoding_type='positional', num_frequencies=4)\n",
    "    elif pe == 'fourier':\n",
    "        mlp = NeuralHighlighter(encoding_type='fourier', sigma=5.0)\n",
    "    elif pe == 'local':\n",
    "        mlp = NeuralHighlighter(encoding_type='local',num_frequencies=8, grid_resolution=16)\n",
    "    elif pe == 'hash':\n",
    "        mlp = NeuralHighlighter(encoding_type='hash', hash_levels=24, hash_features_per_level=4)\n",
    "\n",
    "    mlp.to(device)\n",
    "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "    # ==== CLIP ====\n",
    "    clip_model, preprocess = get_clip_model(clip_model_name)\n",
    "    tokenizer = clip.tokenize\n",
    "\n",
    "    vertices = copy.deepcopy(mesh.vertices).to(device)\n",
    "    n_views = 5\n",
    "    losses = []\n",
    "\n",
    "    # ==== Setup Output Directory ====\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d--%H:%M:%S\")\n",
    "\n",
    "    export_path = os.path.join(base_path, f\"run_PE_{pe}_SYNONYMS_{synonyms_exts}_{timestamp}\")\n",
    "    Path(os.path.join(export_path, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if synonyms_exts:\n",
    "      prompts, weights = generate_prompts(prompt_template, semantic_class, affordance, clip_model, tokenizer, device)\n",
    "      print(prompts)\n",
    "      print(weights)\n",
    "    else:\n",
    "      prompts = [prompt]\n",
    "\n",
    "    start_time = time.time()\n",
    "    # ==== Training Loop ====\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # predict highlight probabilities\n",
    "        if pe == 'hash':\n",
    "          vertices = (vertices - vertices.min(0)[0]) / (vertices.max(0)[0] - vertices.min(0)[0] + 1e-8)\n",
    "        pred_class = mlp(vertices)\n",
    "\n",
    "        # color and render mesh\n",
    "        sampled_mesh = mesh\n",
    "        color_mesh(pred_class, sampled_mesh, colors)\n",
    "        rendered_images, elev, azim = render.render_views(sampled_mesh,\n",
    "                                                          num_views=n_views,\n",
    "                                                          show=False,\n",
    "                                                          center_azim=0,\n",
    "                                                          center_elev=0,\n",
    "                                                          std=1,\n",
    "                                                          return_views=True,\n",
    "                                                          lighting=True,\n",
    "                                                          background=background)\n",
    "\n",
    "        # compute CLIP loss\n",
    "        if (len(prompts)) == 1:\n",
    "          weights = None\n",
    "\n",
    "        loss = clip_loss(rendered_images, prompts, clip_transform, clip_model, tokenizer, device, augment_transform, n_augs, weights)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # report\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iter {i} | Last 100 CLIP score: {np.mean(losses[-100:])} | Current loss: {loss.item()}\")\n",
    "            save_renders(export_path, i, rendered_images)\n",
    "            with open(os.path.join(export_path, \"training_info.txt\"), \"a\") as f:\n",
    "                f.write(f\"Iter {i} | Prompt: {prompts[0]}, Last 100 avg CLIP score: {np.mean(losses[-100:]):.4f}, Current loss: {loss.item():.4f}\\n\")\n",
    "\n",
    "    # ==== Save Final Results ====\n",
    "    save_final_results(export_path, objbase, mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes, seconds = divmod(total_time, 60)\n",
    "\n",
    "    # ==== Save Prompt ====\n",
    "    with open(os.path.join(export_path, \"summary.txt\"), \"w\") as f:\n",
    "        f.write(f\"Prompts:\\n\")\n",
    "        for prompt in prompts:\n",
    "            f.write(f\"{prompt}\\n\")\n",
    "        f.write(f\"CLIP model: {clip_model_name}\\n\")\n",
    "        f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "        f.write(f\"Number of iterations: {n_iter}\\n\")\n",
    "        f.write(f\"Number of views: {n_views}\\n\")\n",
    "        f.write(f\"Number of augmentations: {n_augs}\\n\")\n",
    "        f.write(f\"Final CLIP score: {np.mean(losses[-100:]):.4f}\\n\")\n",
    "        f.write(f\"Final loss: {loss.item():.4f}\\n\")\n",
    "        f.write(f\"Total time: {int(minutes)}m {int(seconds)}s\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mz7MZIymUPoz",
    "outputId": "d817ae41-6016-40d5-c41a-9606f4b616c3"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
