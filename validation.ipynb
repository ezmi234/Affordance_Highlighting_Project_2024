{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
        "!pip install optuna\n",
        "!pip install trimesh\n",
        "!pip install open3d\n",
        "!pip install python-dotenv\n",
        "\n",
        "# Downgrade numpy to a compatible version\n",
        "!pip install numpy==1.23.5 --force-reinstall"
      ],
      "metadata": {
        "id": "FZ1Eoj99pX4b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ezmi234/Affordance_Highlighting_Project_2024.git"
      ],
      "metadata": {
        "id": "EqtNSSDLqb0n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Affordance_Highlighting_Project_2024\n",
        "!git checkout part3-affordancenet-benchmark"
      ],
      "metadata": {
        "id": "3UIrzZVzqfwx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LL-2kfX9uU9"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Show details\n",
        "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UQHH3b39uU9"
      },
      "source": [
        "import clip\n",
        "import kaolin as kal\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from datetime import datetime\n",
        "from kaolin.ops.mesh import face_normals\n",
        "from Normalization import MeshNormalizer\n",
        "from mesh import Mesh\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from utils import color_mesh\n",
        "import pickle\n",
        "from utilities.dataset import load_dataset, get_coordinates, get_affordance_label, is_affordance_present, split_dataset_by_class_and_affordance\n",
        "from utilities.point_cloud import pointcloud_to_voxel_mesh, project_vertex_scores_to_pointcloud, visualize_affordance_pointcloud\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import optuna\n",
        "import gc\n",
        "import pandas as pd\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Z8pejR3WpVwe"
      },
      "cell_type": "code",
      "source": [
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: usually 3 (x, y, z)\n",
        "            hidden_dim: size of hidden layers\n",
        "            output_dim: 2 for [highlight, gray]\n",
        "            num_layers: total number of linear layers\n",
        "        \"\"\"\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "\n",
        "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
        "\n",
        "        for _ in range(num_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm(hidden_dim))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        layers.append(nn.Softmax(dim=1))  # 2-class output\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
        "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
        "    return model, preprocess\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "def clip_loss(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0):\n",
        "    \"\"\"\n",
        "    Computes the CLIP loss as negative cosine similarity between\n",
        "    rendered image embeddings and the text prompt embedding.\n",
        "\n",
        "    Args:\n",
        "        rendered_images (torch.Tensor): shape (B, 3, H, W)\n",
        "        text_prompt (str): e.g., \"a gray chair with highlighted seat\"\n",
        "        clip_transform (torchvision.transforms): preprocessing for CLIP\n",
        "        clip_model (torch.nn.Module): preloaded CLIP model\n",
        "        tokenizer (callable): CLIP tokenizer\n",
        "        device (str): \"cuda\" or \"cpu\"\n",
        "        aug_transform (torchvision.transforms): augmentation for CLIP\n",
        "        n_augs (int): number of augmentations to apply\n",
        "    Returns:\n",
        "        loss (torch.Tensor): scalar CLIP loss\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "\n",
        "    # Encode text\n",
        "    text_tokens = tokenizer([text_prompt]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_tokens).float()\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
        "\n",
        "    if n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        image_features = clip_model.encode_image(clip_image).float()\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Cosine similarity\n",
        "        loss = -torch.mean(torch.cosine_similarity(image_features, text_features))\n",
        "\n",
        "    else:\n",
        "        for _ in range(n_augs):\n",
        "          aug_image = aug_transform(rendered_images)\n",
        "          image_encoded = clip_model.encode_image(aug_image)\n",
        "          loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
        "\n",
        "        loss =  loss / n_augs\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(input_path=\"/content/drive/MyDrive/part3-affordancenet-benchmark/data/\",\n",
        "                    object_class=\"Knife\",\n",
        "                    affordance=\"grasp\",\n",
        "                    val_size=5,\n",
        "                    test_size=5,\n",
        "                    seed=42):\n",
        "    os.makedirs(input_path, exist_ok=True)\n",
        "\n",
        "    # File naming based on class and affordance\n",
        "    val_filename = f\"val_set_{object_class}_{affordance}.pkl\"\n",
        "    test_filename = f\"test_set_{object_class}_{affordance}.pkl\"\n",
        "\n",
        "    val_path = os.path.join(input_path, val_filename)\n",
        "    test_path = os.path.join(input_path, test_filename)\n",
        "\n",
        "    if not os.path.exists(val_path) or not os.path.exists(test_path):\n",
        "        print(\"Dataset files not found in Drive. Checking local storage...\")\n",
        "\n",
        "        if not os.path.exists(\"data/full-shape/full_shape_train_data.pkl\"):\n",
        "            print(\"Local dataset not found. Downloading...\")\n",
        "            !gdown 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF --output full-shape.zip\n",
        "            !unzip -q full-shape.zip -d data/full-shape\n",
        "        else:\n",
        "            print(\"Local dataset found. Skipping download.\")\n",
        "\n",
        "        # Load and split\n",
        "        original_dataset = load_dataset(\"data/full-shape/full_shape_train_data.pkl\")\n",
        "        val_set, test_set = split_dataset_by_class_and_affordance(\n",
        "            original_dataset, object_class, affordance, val_size=val_size, test_size=test_size,  seed=seed\n",
        "        )\n",
        "\n",
        "        # Save to Drive with class and affordance in the name\n",
        "        with open(val_path, \"wb\") as f:\n",
        "            pickle.dump(val_set, f)\n",
        "        with open(test_path, \"wb\") as f:\n",
        "            pickle.dump(test_set, f)\n",
        "\n",
        "        print(f\"Created and saved: {len(val_set)} val / {len(test_set)} test samples for {object_class} - {affordance}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Dataset already exists in Drive. Loading...\")\n",
        "        with open(val_path, \"rb\") as f:\n",
        "            val_set = pickle.load(f)\n",
        "        with open(test_path, \"rb\") as f:\n",
        "            test_set = pickle.load(f)\n",
        "\n",
        "        print(f\"Loaded from Drive: {len(val_set)} val / {len(test_set)} test samples for {object_class} - {affordance}\")\n",
        "\n",
        "    return val_set, test_set\n"
      ],
      "metadata": {
        "id": "zkSmhbWArOBw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_class = \"Table\"\n",
        "affordance = \"support\""
      ],
      "metadata": {
        "id": "M_LiOfDM5blR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val_set, test_set = prepare_dataset(\n",
        "    object_class=semantic_class,\n",
        "    affordance=affordance,\n",
        "    val_size=5,\n",
        "    seed=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "lpz1G-qXtAGK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "MycgOgIgpVwg"
      },
      "cell_type": "code",
      "source": [
        "def get_vertex_scores(pred_class: torch.Tensor, positive_class: int = 1):\n",
        "    \"\"\"\n",
        "    Returns vertex-wise confidence scores for the positive class.\n",
        "\n",
        "    Args:\n",
        "        pred_class (torch.Tensor): shape [N, 2], softmax logits\n",
        "        positive_class (int): which class index should be interpreted as affordance (1 or 0)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: shape [N], probabilities\n",
        "    \"\"\"\n",
        "    probs = F.softmax(pred_class, dim=1)\n",
        "    return probs[:, positive_class]\n",
        "\n",
        "def compute_IoU(pred_labels: torch.Tensor, gt_labels: torch.Tensor):\n",
        "    pred = pred_labels.bool()\n",
        "    gt = gt_labels.bool()\n",
        "    intersection = (pred & gt).sum().float()\n",
        "    union = (pred | gt).sum().float()\n",
        "    return (intersection / union).item() if union > 0 else float('nan')\n",
        "\n",
        "def optimize_IoU_threshold(projected_scores, gt_labels, thresholds=None, gt_thresholds=None):\n",
        "    \"\"\"\n",
        "    Computes IoU over different thresholds on predicted scores and GT thresholds.\n",
        "\n",
        "    Args:\n",
        "        projected_scores (torch.Tensor): shape [N], soft prediction per point\n",
        "        gt_labels (torch.Tensor): shape [N], soft or binary GT labels\n",
        "        thresholds (list or tensor): list of thresholds to test (default: 0.1 to 0.9)\n",
        "        gt_thresholds (list or tensor): list of GT thresholds to test (default: 0.0 to 0.5)\n",
        "\n",
        "    Returns:\n",
        "        (float, float, float, pd.DataFrame): best prediction threshold, best GT threshold, best IoU, DataFrame of all results\n",
        "    \"\"\"\n",
        "    if thresholds is None:\n",
        "        thresholds = torch.linspace(0.1, 0.9, steps=9)\n",
        "    if gt_thresholds is None:\n",
        "        gt_thresholds = torch.linspace(0.0, 0.5, steps=6)\n",
        "\n",
        "    records = []\n",
        "    best_iou = -1\n",
        "    best_pred_thresh = 0.5\n",
        "    best_gt_thresh = 0.0\n",
        "\n",
        "    for gt_t in gt_thresholds:\n",
        "        gt_binary = (gt_labels > gt_t).long()\n",
        "        for pred_t in thresholds:\n",
        "            pred_binary = (projected_scores > pred_t).long()\n",
        "            iou = compute_IoU(pred_binary, gt_binary)\n",
        "            records.append({\"pred_threshold\": float(pred_t), \"gt_threshold\": float(gt_t), \"iou\": iou})\n",
        "            if iou >= best_iou:\n",
        "                best_iou = iou\n",
        "                best_pred_thresh = float(pred_t)\n",
        "                best_gt_thresh = float(gt_t)\n",
        "\n",
        "    return best_pred_thresh, best_gt_thresh, best_iou, pd.DataFrame(records)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Memory Cleanup Function ===\n",
        "def clean_memory(*objects):\n",
        "    \"\"\"\n",
        "    Frees GPU memory and runs garbage collection\n",
        "    \"\"\"\n",
        "    for obj in objects:\n",
        "        del obj\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "JaTeKbQLoeIK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HBR_1SCppVwg"
      },
      "cell_type": "code",
      "source": [
        "def optimize_highlighting(sample, affordance, mlp, optimizer, render, clip_model, tokenizer,\n",
        "                        clip_transform, augment_transform, n_views, n_augs, n_iter, colors, background,\n",
        "                        output_dir, device, run_n, prompt, resolution=16, voxel_threshold=0.5):\n",
        "\n",
        "    # ==== Set Seed for Determinism ====\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    export_path = os.path.join(output_dir, f\"run_{run_n}_{str(sample['semantic class']).lower()}_{affordance}\")\n",
        "    os.makedirs(os.path.join(export_path, \"renders\"), exist_ok=True)\n",
        "    temp_obj_path = \"data/outputDemo.obj\"\n",
        "\n",
        "    if \"semantic class\" not in sample or not isinstance(sample[\"semantic class\"], str):\n",
        "        raise ValueError(f\"Error: Missing or invalid 'semantic class' field in sample: {sample}\")\n",
        "\n",
        "    points = get_coordinates(sample, device)\n",
        "    trimesh_mesh = pointcloud_to_voxel_mesh(\n",
        "        points,\n",
        "        resolution=resolution,\n",
        "        threshold=voxel_threshold,\n",
        "        export_path=temp_obj_path\n",
        "    )\n",
        "\n",
        "    sampled_mesh = Mesh(temp_obj_path)\n",
        "    MeshNormalizer(sampled_mesh)()\n",
        "    vertices = sampled_mesh.vertices.clone().detach().to(device).float()\n",
        "\n",
        "    losses = []\n",
        "    pred_class = None\n",
        "    start_time = time.time()\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        optimizer.zero_grad()\n",
        "        pred_class = mlp(vertices)\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "\n",
        "        rendered_images, elev, azim = render.render_views(\n",
        "            sampled_mesh,\n",
        "            num_views=n_views,\n",
        "            show=False,\n",
        "            center_azim=0,\n",
        "            center_elev=0,\n",
        "            std=1,\n",
        "            return_views=True,\n",
        "            lighting=True,\n",
        "            background=background\n",
        "        )\n",
        "\n",
        "        loss = clip_loss(rendered_images, prompt, clip_transform, clip_model, tokenizer, device, augment_transform, n_augs)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            # print(f\"Last 100 CLIP score: {np.mean(losses[-100:])}\")\n",
        "            save_renders(export_path, i, rendered_images)\n",
        "            with open(os.path.join(export_path, \"log.txt\"), \"a\") as f:\n",
        "                f.write(f\"Iter {i} | Loss: {loss.item():.4f} | Last 100 avg CLIP score: {np.mean(losses[-100:]):.4f}\\n\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    minutes, seconds = divmod(total_time, 60)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Save final output\n",
        "    save_final_results(export_path, sample[\"semantic class\"], sampled_mesh, mlp, vertices, colors, render, background)\n",
        "    with open(os.path.join(export_path, \"final_score.txt\"), \"w\") as f:\n",
        "        f.write(f\"Prompt: {prompt}\\n\")\n",
        "        f.write(f\"Final average CLIP loss: {sum(losses[-10:]) / 10:.4f}\\n\")\n",
        "        f.write(f\"Total time: {int(minutes)}m {int(seconds)}s\\n\")\n",
        "        f.write(f\"Resolution: {resolution}\\n\")\n",
        "        f.write(f\"Voxel threshold: {voxel_threshold}\\n\")\n",
        "        f.write(f\"Number of views: {n_views}\\n\")\n",
        "        f.write(f\"Number of augmentations: {n_augs}\\n\")\n",
        "        f.write(f\"Number of iterations: {n_iter}\\n\")\n",
        "        f.write(f\"Learning rate: {optimizer.param_groups[0]['lr']}\\n\")\n",
        "\n",
        "    if os.path.exists(temp_obj_path):\n",
        "        os.remove(temp_obj_path)\n",
        "\n",
        "    return trimesh_mesh, pred_class, export_path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6AJN8GSOpVwg"
      },
      "cell_type": "code",
      "source": [
        "# Optuna objective function for affordance highlighting validation\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # === Sample hyperparameters ===\n",
        "    clip_model_name = trial.suggest_categorical(\"clip_model\", [\"ViT-B/32\", \"ViT-B/16\", \"ViT-L/14\"])\n",
        "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
        "    network_depth = trial.suggest_int(\"depth\", 3, 8)\n",
        "    n_augs = trial.suggest_int(\"n_augs\", 0, 5)\n",
        "    n_views = trial.suggest_int(\"n_views\", 2, 8)\n",
        "    n_iters = trial.suggest_int(\"n_iter\", 1600, 2800, step=100)\n",
        "    resolution = trial.suggest_int(\"resolution\", 8, 24, step=8)\n",
        "    threshold = trial.suggest_float(\"voxel_thresh\", 0.1, 0.9)\n",
        "    prompt_template = trial.suggest_categorical(\"prompt\", [\n",
        "        \"A 3D render of a gray {} with highlighted {}\",\n",
        "        \"A gray {} with highlighted {}\",\n",
        "        \"A gray {} with highlighted {} region\",\n",
        "        \"An highlighted {} region with gray {}\"\n",
        "    ])\n",
        "\n",
        "    # ==== Device ====\n",
        "    render_res = 224\n",
        "    render = Renderer(dim=(render_res, render_res))\n",
        "\n",
        "    # ==== Normalization and Augmentation ====\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((render_res, render_res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # ==== Colors and Other Constants ====\n",
        "    colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
        "    background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "    # === Setup output directory ===\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    run_name = f\"trial_{trial.number}_{clip_model_name.replace('/', '-')}_res{resolution}_th{threshold:.2f}_lr{learning_rate:.0e}_depth{network_depth}_views{n_views}_augs{n_augs}_iters{n_iters}\"\n",
        "    export_path = f\"/content/drive/MyDrive/part3-affordancenet-benchmark/outputs/{run_name}\"\n",
        "\n",
        "    # === Load models and transforms ===\n",
        "    clip_model, _ = get_clip_model(clip_model_name)\n",
        "    tokenizer = clip.tokenize\n",
        "\n",
        "    # === Render and MLP ===\n",
        "    render_res = 224\n",
        "    render = Renderer(dim=(render_res, render_res))\n",
        "\n",
        "    total_ious = []\n",
        "    results = []\n",
        "\n",
        "    cont = 0;\n",
        "    for sample in val_set:\n",
        "        if is_affordance_present(sample, affordance):\n",
        "            if prompt_template == \"An highlighted {} region with gray {}\":\n",
        "              prompt = prompt_template.format(affordance.lower(), str(sample['semantic class']).lower())\n",
        "            else:\n",
        "              prompt = prompt_template.format(str(sample['semantic class']).lower(), affordance.lower())\n",
        "\n",
        "            mlp = NeuralHighlighter(num_layers=network_depth).to(device)\n",
        "            optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "            mesh, pred_class, render_dir = optimize_highlighting(\n",
        "                sample=sample,\n",
        "                affordance=affordance,\n",
        "                mlp=mlp,\n",
        "                optimizer=optimizer,\n",
        "                render=render,\n",
        "                clip_model=clip_model,\n",
        "                tokenizer=tokenizer,\n",
        "                clip_transform=clip_transform,\n",
        "                augment_transform=augment_transform,\n",
        "                n_views=n_views,\n",
        "                n_augs=n_augs,\n",
        "                n_iter=n_iters,\n",
        "                colors=colors,\n",
        "                background=background,\n",
        "                run_n=cont,\n",
        "                output_dir=export_path,\n",
        "                device=device,\n",
        "                prompt=prompt,\n",
        "                resolution=resolution,\n",
        "                voxel_threshold=threshold,\n",
        "            )\n",
        "\n",
        "            pointcloud = get_coordinates(sample, device)\n",
        "            gt_labels = get_affordance_label(sample, affordance, device)\n",
        "            vertex_scores = get_vertex_scores(pred_class, positive_class=0)\n",
        "            projected_scores = project_vertex_scores_to_pointcloud(mesh, vertex_scores, pointcloud)\n",
        "\n",
        "            best_pred_thresh, best_gt_thresh, best_iou, iou_df = optimize_IoU_threshold(\n",
        "                projected_scores,\n",
        "                gt_labels,\n",
        "                thresholds=torch.linspace(0.1, 0.9, steps=9),\n",
        "                gt_thresholds=torch.linspace(0.0, 0.45, steps=9)\n",
        "            )\n",
        "\n",
        "            trial.report(best_iou, step=cont)\n",
        "            if trial.should_prune():\n",
        "              clean_memory(mlp, optimizer, render, clip_model, tokenizer, pointcloud, gt_labels, vertex_scores)\n",
        "              raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "            total_ious.append(best_iou)\n",
        "\n",
        "            results.append({\n",
        "                **trial.params,\n",
        "                \"sample_class\": sample['semantic class'],\n",
        "                \"affordance\": affordance,\n",
        "                \"best_iou\": best_iou,\n",
        "                \"best_pred_threshold\": best_pred_thresh,\n",
        "                \"best_gt_threshold\": best_gt_thresh\n",
        "            })\n",
        "\n",
        "            with open(os.path.join(render_dir, f\"projected_scores_{sample['semantic class']}.pkl\"), \"wb\") as f:\n",
        "                pickle.dump(projected_scores.detach().cpu().numpy(), f)\n",
        "            iou_df.to_csv(os.path.join(render_dir, f\"iou_by_threshold_{sample['semantic class']}.csv\"), index=False)\n",
        "\n",
        "            cont += 1\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(os.path.join(export_path, \"metrics.csv\"), index=False)\n",
        "\n",
        "    mean_iou = np.nanmean(total_ious)\n",
        "    print(f\"Trial {trial.number} finished with mean IoU: {mean_iou:.4f}\")\n",
        "    return mean_iou"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "export_path = \"/content/drive/MyDrive/part3-affordancenet-benchmark/\"\n",
        "db_folder = os.path.join(export_path, \"db\")\n",
        "os.makedirs(db_folder, exist_ok=True)\n",
        "\n",
        "# Use local storage\n",
        "db_path = os.path.join(db_folder, \"optuna_{}_{}.db\".format(semantic_class.lower(), affordance))\n",
        "storage_url = f\"sqlite:///{db_path}\"\n",
        "\n",
        "# Use a remote PostgreSQL database for Optuna storage\n",
        "# env_path = \"/content/drive/MyDrive/.env\"\n",
        "# load_dotenv(dotenv_path=env_path)\n",
        "# storage_url = os.getenv(\"STORAGE_URL\")\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=\"affordance_highlighting_part3\",\n",
        "    storage=storage_url,\n",
        "    load_if_exists=True\n",
        ")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "os.makedirs(os.path.join(export_path, \"outputs\"), exist_ok=True)\n",
        "pd.DataFrame([study.best_params]).to_csv(\n",
        "    os.path.join(export_path, \"outputs\", \"best_hyperparameters.csv\"), index=False\n",
        ")\n",
        "\n",
        "# Print the best trial\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "12tQf_M-5LW8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## Clear torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "qXh7DE9mGSp0"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}