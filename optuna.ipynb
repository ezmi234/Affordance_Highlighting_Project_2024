{
 "cells": [
  {
   "metadata": {
    "id": "94Z1Mc-Cb7gx"
   },
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
    "!pip install optuna\n",
    "\n",
    "# Downgrade numpy to a compatible version\n",
    "!pip install numpy==1.23.5 --force-reinstall"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "T91Neq2bqmNc"
   },
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/ezmi234/Affordance_Highlighting_Project_2024.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1xgV2CegqmNc"
   },
   "cell_type": "code",
   "source": [
    "%cd Affordance_Highlighting_Project_2024\n",
    "!git checkout part1-mesh-highlighter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "CxXRzyRBqmNd"
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4ozrcjricJCs"
   },
   "source": [
    "import clip\n",
    "import copy\n",
    "import kaolin as kal\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from render import Renderer\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import optuna\n",
    "import time\n",
    "import gc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "clip_models = clip.available_models()\n",
    "print(\"Available CLIP models:\")\n",
    "for m in clip_models:\n",
    "    print(m)"
   ],
   "metadata": {
    "id": "mX8ZkywmmfTK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tPJnFsqzcr79"
   },
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))  # 2-class output\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "def clip_loss(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0):\n",
    "    \"\"\"\n",
    "    Computes the CLIP loss as negative cosine similarity between\n",
    "    rendered image embeddings and the text prompt embedding.\n",
    "\n",
    "    Args:\n",
    "        rendered_images (torch.Tensor): shape (B, 3, H, W)\n",
    "        text_prompt (str): e.g., \"a gray chair with highlighted seat\"\n",
    "        clip_transform (torchvision.transforms): preprocessing for CLIP\n",
    "        clip_model (torch.nn.Module): preloaded CLIP model\n",
    "        tokenizer (callable): CLIP tokenizer\n",
    "        device (str): \"cuda\" or \"cpu\"\n",
    "        aug_transform (torchvision.transforms): augmentation for CLIP\n",
    "        n_augs (int): number of augmentations to apply\n",
    "    Returns:\n",
    "        loss (torch.Tensor): scalar CLIP loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # Encode text\n",
    "    text_tokens = tokenizer([text_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        image_features = clip_model.encode_image(clip_image).float()\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        loss = -torch.mean(torch.cosine_similarity(image_features, text_features))\n",
    "\n",
    "    else:\n",
    "        for _ in range(n_augs):\n",
    "          aug_image = aug_transform(rendered_images)\n",
    "          image_encoded = clip_model.encode_image(aug_image)\n",
    "          loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
    "\n",
    "        loss =  loss / n_augs\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==== Settings ====\n",
    "render_res = 224\n",
    "obj_path = 'data/dog.obj'\n",
    "prompt = 'A gray dog with highlighted hat'"
   ],
   "metadata": {
    "id": "lGFvH8hAShaG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==== Load Mesh ====\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# ==== Normalization and Augmentation ====\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((render_res, render_res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "vertices = copy.deepcopy(mesh.vertices).to(device)\n",
    "n_views = 5"
   ],
   "metadata": {
    "id": "duDsGJbVW3SK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    # Constrain most sources of randomness\n",
    "    # (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "    # ==== Set Seed for Determinism ====\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    clip_model_name = trial.suggest_categorical(\"clip_model\", [\"ViT-B/32\", \"ViT-B/16\", \"ViT-L/14\"])\n",
    "    safe_model_name = clip_model_name.replace(\"/\", \"-\")\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    network_depth = trial.suggest_int(\"depth\", 3, 8)\n",
    "    n_augs = trial.suggest_int(\"n_augs\", 0, 5)\n",
    "    n_views = trial.suggest_int(\"n_views\", 2, 8)\n",
    "    n_iters = trial.suggest_int(\"n_iters\", 1500, 3000, step=100)\n",
    "\n",
    "    # Timestamped export path to Google Drive\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"trial_{trial.number}_{safe_model_name}_lr{learning_rate:.1e}_d{network_depth}_v{n_views}_a{n_augs}_i{n_iters}\"\n",
    "    export_path = f\"/content/drive/MyDrive/affordance_outputs/optuna_{timestamp}_{run_name}\"\n",
    "    os.makedirs(export_path+\"/renders\", exist_ok=True)\n",
    "\n",
    "    # === Load components ===\n",
    "    model, preprocess = get_clip_model(clip_model_name)\n",
    "    tokenizer = clip.tokenize\n",
    "\n",
    "    # Define MLP with trial's depth\n",
    "    mlp = NeuralHighlighter(num_layers=network_depth).to(device)\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    for i in range(n_iters):\n",
    "        optimizer.zero_grad()\n",
    "        pred_class = mlp(vertices)\n",
    "\n",
    "        color_mesh(pred_class, mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(\n",
    "            mesh,\n",
    "            num_views=n_views,\n",
    "            show=False,\n",
    "            center_azim=0,\n",
    "            center_elev=0,\n",
    "            std=1,\n",
    "            return_views=True,\n",
    "            lighting=True,\n",
    "            background=background\n",
    "        )\n",
    "\n",
    "        loss = clip_loss(rendered_images, prompt, clip_transform, model, tokenizer, device, augment_transform, n_augs)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Save intermediate results every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            save_renders(export_path, i, rendered_images)\n",
    "            with open(os.path.join(export_path, \"log.txt\"), \"a\") as f:\n",
    "                f.write(f\"Iter {i} | Loss: {loss.item():.4f} | Last 100 avg CLIP score: {np.mean(losses[-100:]):.4f}\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    minutes, seconds = divmod(total_time, 60)\n",
    "\n",
    "    del model  # if you create a model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Save final output\n",
    "    save_final_results(export_path, run_name, mesh, mlp, vertices, colors, render, background)\n",
    "    with open(os.path.join(export_path, \"final_score.txt\"), \"w\") as f:\n",
    "        f.write(f\"Prompt: {prompt}\\n\")\n",
    "        f.write(f\"Final average CLIP loss: {sum(losses[-100:]) / 100:.4f}\\n\")\n",
    "        f.write(f\"Total time: {int(minutes)}m {int(seconds)}s\\n\")\n",
    "        f.write(str(trial.params))\n",
    "\n",
    "    return sum(losses[-100:]) / 100  # use average of last 100 iterations as final score"
   ],
   "metadata": {
    "id": "0kuuQif6QWXz"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ],
   "metadata": {
    "id": "DBeDkDSdQa3e"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
