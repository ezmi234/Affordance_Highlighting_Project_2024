{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LL-2kfX9uU9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1847d7dd-34c8-47ac-8d09-c7203234afe3",
    "ExecuteTime": {
     "end_time": "2025-05-03T15:41:16.405326Z",
     "start_time": "2025-05-03T15:41:15.407553Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124, CUDA version: 12.4, GPU available: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1UQHH3b39uU9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "outputId": "b6a8e90e-94e8-4855-b3cc-07299f5f1850",
    "ExecuteTime": {
     "end_time": "2025-05-03T16:26:58.230898Z",
     "start_time": "2025-05-03T16:26:58.227368Z"
    }
   },
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh as mesh\n",
    "import kaolin.ops.conversions as conversions\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import permutations, product\n",
    "from kaolin.ops.mesh import face_normals\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import pickle\n",
    "from scipy.spatial import cKDTree\n",
    "from utilities.dataset import load_dataset, get_coordinates\n",
    "from utilities.point_cloud import pointcloud_to_voxel_mesh, project_mesh_labels_to_pointcloud_torch, visualize_affordance_pointcloud"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T15:48:05.874234Z",
     "start_time": "2025-05-03T15:48:05.865863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))  # 2-class output\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "def clip_loss(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # Encode text\n",
    "    text_tokens = tokenizer([text_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        image_features = clip_model.encode_image(clip_image).float()\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        loss = -torch.mean(torch.cosine_similarity(image_features, text_features))\n",
    "\n",
    "    else:\n",
    "        for _ in range(n_augs):\n",
    "          aug_image = aug_transform(rendered_images)\n",
    "          image_encoded = clip_model.encode_image(aug_image)\n",
    "          loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
    "\n",
    "        loss =  loss / n_augs\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dluhTREE9uU_",
    "ExecuteTime": {
     "end_time": "2025-05-03T15:48:09.168770Z",
     "start_time": "2025-05-03T15:48:09.165368Z"
    }
   },
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "# ==== Set Seed for Determinism ====\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def load_vertices(data):\n",
    "  if type(data) == str:\n",
    "    mesh = o3d.io.read_triangle_mesh(data)\n",
    "    mesh.compute_vertex_normals()\n",
    "    vertices = mesh.sample_points_uniformly(number_of_points=4096)\n",
    "    return torch.tensor(np.asarray(vertices.points), dtype=torch.float32).to(device)\n",
    "  else:\n",
    "    return torch.tensor(data, dtype=torch.float32).to(device)"
   ],
   "metadata": {
    "id": "erYMIwWHtrx8",
    "ExecuteTime": {
     "end_time": "2025-05-03T15:48:21.916708Z",
     "start_time": "2025-05-03T15:48:21.914353Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LwpTp2c59uU_",
    "ExecuteTime": {
     "end_time": "2025-05-03T15:49:02.621480Z",
     "start_time": "2025-05-03T15:49:02.618961Z"
    }
   },
   "source": [
    "# ==== Hyperparameters and Settings ====\n",
    "render_res = 224\n",
    "learning_rate = 0.00005\n",
    "n_iter = 2200\n",
    "n_augs = 1\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-B/16'"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zBIP-dE69uU_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cc4a00be-3a16-4354-96fe-51b1c9781986",
    "ExecuteTime": {
     "end_time": "2025-05-03T15:49:31.990461Z",
     "start_time": "2025-05-03T15:49:29.924544Z"
    }
   },
   "source": [
    "# ==== Device ====\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "\n",
    "# ==== CLIP ====\n",
    "clip_model, preprocess = get_clip_model(clip_model_name)\n",
    "tokenizer = clip.tokenize\n",
    "\n",
    "# ==== Normalization and Augmentation ====\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((render_res, render_res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# ==== Neural Highlighter ====\n",
    "mlp = NeuralHighlighter().to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "n_views = 7\n",
    "losses = []"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: ViT-B/16 on cuda (jit=False)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"data/full-shape/full_shape_train_data.pkl\")"
   ],
   "metadata": {
    "id": "MhhPReA0_nWu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b03b7e7f-63db-4f2e-d213-3b9e399f051b",
    "ExecuteTime": {
     "end_time": "2025-05-03T16:15:06.262441Z",
     "start_time": "2025-05-03T16:15:04.738524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_data\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": [
    "affordances = ['grasp', 'wrap grasp', 'pull']\n",
    "\n",
    "random.shuffle(dataset)\n",
    "#n_elements = int(0.01 * len(dataset))\n",
    "n_elements = 1\n",
    "val_set = dataset[:n_elements]\n",
    "test_set = dataset[n_elements:(n_elements*2)]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JYk66K3WoDP",
    "outputId": "9951d853-a62a-429d-a3c5-11099600c175",
    "ExecuteTime": {
     "end_time": "2025-05-03T15:34:32.045677Z",
     "start_time": "2025-05-03T15:34:32.039303Z"
    }
   },
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:15:10.156982Z",
     "start_time": "2025-05-03T16:15:10.153542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_counts = {}\n",
    "test_counts = {}\n",
    "\n",
    "for item in val_set:\n",
    "    cls = item[\"semantic class\"]\n",
    "    if cls not in val_counts:\n",
    "        val_counts[cls] = 0\n",
    "    val_counts[cls] += 1\n",
    "\n",
    "for item in test_set:\n",
    "    cls = item[\"semantic class\"]\n",
    "    if cls not in test_counts:\n",
    "        test_counts[cls] = 0\n",
    "    test_counts[cls] += 1\n",
    "\n",
    "sorted_val = sorted(val_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_test = sorted(test_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Validation Set Semantic Class Counts:\")\n",
    "for cls, count in sorted_val:\n",
    "    print(f\"{cls}: {count}\")\n",
    "\n",
    "print(\"\\nTest Set Semantic Class Counts:\")\n",
    "for cls, count in sorted_test:\n",
    "    print(f\"{cls}: {count}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Semantic Class Counts:\n",
      "Chair: 1\n",
      "\n",
      "Test Set Semantic Class Counts:\n",
      "Chair: 1\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:15:12.841701Z",
     "start_time": "2025-05-03T16:15:12.839756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_prompt(semantic_class, affordance):\n",
    "  temp_prompt = \"a gray \" + semantic_class + \" with highlighted \" + affordance + \" region\"\n",
    "  return temp_prompt"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:25:00.412184Z",
     "start_time": "2025-05-03T16:25:00.406785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_highlighting(sample, affordances, mlp, optimizer, render, clip_model, tokenizer, clip_transform, augment_transform, n_augs, n_iter, colors, background, output_dir, device):\n",
    "    \"\"\"\n",
    "    Optimizes the highlighting process for a given sample and affordances.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): The input sample containing mesh and semantic class information.\n",
    "        affordances (list): List of affordances to highlight.\n",
    "        mlp (nn.Module): Neural network model for highlighting.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training the model.\n",
    "        render (Renderer): Renderer for generating views of the mesh.\n",
    "        clip_model (nn.Module): CLIP model for computing loss.\n",
    "        tokenizer (function): Tokenizer for text prompts.\n",
    "        clip_transform (transforms.Compose): Transformations for CLIP input images.\n",
    "        augment_transform (transforms.Compose): Augmentation transformations for images.\n",
    "        n_augs (int): Number of augmentations for CLIP loss.\n",
    "        n_iter (int): Number of optimization iterations.\n",
    "        colors (torch.Tensor): Tensor of colors for highlighting.\n",
    "        background (torch.Tensor): Background color tensor.\n",
    "        output_dir (str): Directory to save results.\n",
    "        device (torch.device): Device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicted class probabilities for vertices.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    export_path = os.path.join(output_dir, f\"run_{timestamp}\")\n",
    "    os.makedirs(os.path.join(export_path, \"renders\"), exist_ok=True)\n",
    "    temp_obj_path = \"data/outputDemo.obj\"\n",
    "\n",
    "    if \"semantic class\" not in sample or not isinstance(sample[\"semantic class\"], str):\n",
    "        raise ValueError(f\"Error: Missing or invalid 'semantic class' field in sample: {sample}\")\n",
    "\n",
    "    prompt = build_prompt(sample[\"semantic class\"], affordances[0])  # Use the first affordance for simplicity\n",
    "\n",
    "    points = get_coordinates(sample, device)\n",
    "    mesh = pointcloud_to_voxel_mesh(\n",
    "        points,  # sampled point cloud from Open3D\n",
    "        resolution=16,\n",
    "        threshold=0.5,\n",
    "        export_path=temp_obj_path\n",
    "    )\n",
    "\n",
    "\n",
    "    sampled_mesh = Mesh(temp_obj_path)\n",
    "    MeshNormalizer(sampled_mesh)()\n",
    "    vertices = sampled_mesh.vertices.clone().detach().to(device).float()\n",
    "\n",
    "    losses = []\n",
    "    pred_class = None\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        optimizer.zero_grad()\n",
    "        pred_class = mlp(vertices)  # Predict highlight probabilities\n",
    "        color_mesh(pred_class, sampled_mesh, colors)  # Color mesh\n",
    "\n",
    "        # Render and compute loss\n",
    "        rendered_images, elev, azim = render.render_views(\n",
    "            sampled_mesh,\n",
    "            num_views=n_views,\n",
    "            show=False,\n",
    "            center_azim=0,\n",
    "            center_elev=0,\n",
    "            std=1,\n",
    "            return_views=True,\n",
    "            lighting=True,\n",
    "            background=background\n",
    "        )\n",
    "        loss = clip_loss(rendered_images, prompt, clip_transform, clip_model, tokenizer, device, augment_transform, n_augs)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Save the loss for logging\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Log and save intermediate results\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Last 100 CLIP score: {np.mean(losses[-100:])}\")\n",
    "            save_renders(export_path, i, rendered_images)\n",
    "            with open(os.path.join(export_path, \"training_info.txt\"), \"a\") as f:\n",
    "                f.write(f\"Iter {i}: Prompt: {prompt}, Avg CLIP score: {np.mean(losses[-100:])}, CLIP score: {loss.item()}\\n\")\n",
    "\n",
    "    # Final save and cleanup\n",
    "    save_final_results(export_path, sample[\"semantic class\"], sampled_mesh, mlp, vertices, colors, render, background)\n",
    "    with open(os.path.join(export_path, \"prompt.txt\"), \"w\") as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "    if os.path.exists(temp_obj_path):\n",
    "        os.remove(temp_obj_path)\n",
    "\n",
    "    return mesh, pred_class"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:30:56.163244Z",
     "start_time": "2025-05-03T16:29:47.886308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sample in val_set:\n",
    "  for affordance in affordances[:1]:\n",
    "    mesh, pred_class = optimize_highlighting(\n",
    "        sample=sample,\n",
    "        affordances=[affordance],\n",
    "        mlp=mlp,\n",
    "        optimizer=optim,\n",
    "        render=render,\n",
    "        clip_model=clip_model,\n",
    "        tokenizer=tokenizer,\n",
    "        clip_transform=clip_transform,\n",
    "        augment_transform=augment_transform,\n",
    "        n_augs=n_augs,\n",
    "        n_iter=n_iter,\n",
    "        colors=colors,\n",
    "        background=background,\n",
    "        output_dir=output_dir,\n",
    "        device=device\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "Unexpected type passed to requires_grad None\n",
      "Attribute \"vertex_normals\" has not been set and failed to be computed due to: 'NoneType' object has no attribute 'unsqueeze'\n",
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "  0%|          | 2/2200 [00:00<02:56, 12.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2911253571510315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 102/2200 [00:06<02:24, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.26794628441333773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 202/2200 [00:13<02:14, 14.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2676777197420597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 302/2200 [00:20<02:13, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.26775323927402495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 404/2200 [00:28<02:00, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.268311939239502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 503/2200 [00:36<02:41, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.26739155292510985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 603/2200 [00:44<02:11, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.266822796612978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 703/2200 [00:51<01:41, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.267678948789835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 803/2200 [00:57<01:36, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2671652664244175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 903/2200 [01:05<01:41, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.27071956887841225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 940/2200 [01:07<01:31, 13.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[76]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m val_set:\n\u001B[32m      2\u001B[39m   \u001B[38;5;28;01mfor\u001B[39;00m affordance \u001B[38;5;129;01min\u001B[39;00m affordances[:\u001B[32m1\u001B[39m]:\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     mesh, pred_class = \u001B[43moptimize_highlighting\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[43msample\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m        \u001B[49m\u001B[43maffordances\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43maffordance\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrender\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclip_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclip_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclip_transform\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclip_transform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[43maugment_transform\u001B[49m\u001B[43m=\u001B[49m\u001B[43maugment_transform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_augs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_augs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcolors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbackground\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbackground\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[70]\u001B[39m\u001B[32m, line 69\u001B[39m, in \u001B[36moptimize_highlighting\u001B[39m\u001B[34m(sample, affordances, mlp, optimizer, render, clip_model, tokenizer, clip_transform, augment_transform, n_augs, n_iter, colors, background, output_dir, device)\u001B[39m\n\u001B[32m     57\u001B[39m \u001B[38;5;66;03m# Render and compute loss\u001B[39;00m\n\u001B[32m     58\u001B[39m rendered_images, elev, azim = render.render_views(\n\u001B[32m     59\u001B[39m     sampled_mesh,\n\u001B[32m     60\u001B[39m     num_views=n_views,\n\u001B[32m   (...)\u001B[39m\u001B[32m     67\u001B[39m     background=background\n\u001B[32m     68\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m loss = \u001B[43mclip_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrendered_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_transform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maugment_transform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_augs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m loss.backward(retain_graph=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     71\u001B[39m optimizer.step()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 87\u001B[39m, in \u001B[36mclip_loss\u001B[39m\u001B[34m(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform, n_augs)\u001B[39m\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_augs):\n\u001B[32m     86\u001B[39m   aug_image = aug_transform(rendered_images)\n\u001B[32m---> \u001B[39m\u001B[32m87\u001B[39m   image_encoded = \u001B[43mclip_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43maug_image\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     88\u001B[39m   loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n\u001B[32m     90\u001B[39m loss =  loss / n_augs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/clip/model.py:341\u001B[39m, in \u001B[36mCLIP.encode_image\u001B[39m\u001B[34m(self, image)\u001B[39m\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mencode_image\u001B[39m(\u001B[38;5;28mself\u001B[39m, image):\n\u001B[32m--> \u001B[39m\u001B[32m341\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/clip/model.py:232\u001B[39m, in \u001B[36mVisionTransformer.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    229\u001B[39m x = \u001B[38;5;28mself\u001B[39m.ln_pre(x)\n\u001B[32m    231\u001B[39m x = x.permute(\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m)  \u001B[38;5;66;03m# NLD -> LND\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    233\u001B[39m x = x.permute(\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m)  \u001B[38;5;66;03m# LND -> NLD\u001B[39;00m\n\u001B[32m    235\u001B[39m x = \u001B[38;5;28mself\u001B[39m.ln_post(x[:, \u001B[32m0\u001B[39m, :])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/clip/model.py:203\u001B[39m, in \u001B[36mTransformer.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor):\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mresblocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/clip/model.py:190\u001B[39m, in \u001B[36mResidualAttentionBlock.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor):\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     x = x + \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mln_1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m     x = x + \u001B[38;5;28mself\u001B[39m.mlp(\u001B[38;5;28mself\u001B[39m.ln_2(x))\n\u001B[32m    192\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/clip/model.py:187\u001B[39m, in \u001B[36mResidualAttentionBlock.attention\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    185\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mattention\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor):\n\u001B[32m    186\u001B[39m     \u001B[38;5;28mself\u001B[39m.attn_mask = \u001B[38;5;28mself\u001B[39m.attn_mask.to(dtype=x.dtype, device=x.device) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.attn_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1373\u001B[39m, in \u001B[36mMultiheadAttention.forward\u001B[39m\u001B[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[39m\n\u001B[32m   1347\u001B[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001B[32m   1348\u001B[39m         query,\n\u001B[32m   1349\u001B[39m         key,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1370\u001B[39m         is_causal=is_causal,\n\u001B[32m   1371\u001B[39m     )\n\u001B[32m   1372\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1373\u001B[39m     attn_output, attn_output_weights = \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1374\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1375\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1376\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1377\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1378\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1379\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1380\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1381\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1382\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1383\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1384\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1385\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mout_proj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1386\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mout_proj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1387\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1388\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1389\u001B[39m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1390\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1391\u001B[39m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1393\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1394\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.batch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[32m   1395\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output.transpose(\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m), attn_output_weights\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/functional.py:6230\u001B[39m, in \u001B[36mmulti_head_attention_forward\u001B[39m\u001B[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[39m\n\u001B[32m   6226\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_separate_proj_weight:\n\u001B[32m   6227\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[32m   6228\u001B[39m         in_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   6229\u001B[39m     ), \u001B[33m\"\u001B[39m\u001B[33muse_separate_proj_weight is False but in_proj_weight is None\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m6230\u001B[39m     q, k, v = \u001B[43m_in_projection_packed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_proj_bias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6231\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   6232\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[32m   6233\u001B[39m         q_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   6234\u001B[39m     ), \u001B[33m\"\u001B[39m\u001B[33muse_separate_proj_weight is True but q_proj_weight is None\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5618\u001B[39m, in \u001B[36m_in_projection_packed\u001B[39m\u001B[34m(q, k, v, w, b)\u001B[39m\n\u001B[32m   5614\u001B[39m     proj = linear(q, w, b)\n\u001B[32m   5615\u001B[39m     \u001B[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001B[39;00m\n\u001B[32m   5616\u001B[39m     proj = (\n\u001B[32m   5617\u001B[39m         \u001B[43mproj\u001B[49m\u001B[43m.\u001B[49m\u001B[43munflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m5618\u001B[39m \u001B[43m        \u001B[49m\u001B[43m.\u001B[49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   5619\u001B[39m         .transpose(\u001B[32m0\u001B[39m, -\u001B[32m2\u001B[39m)\n\u001B[32m   5620\u001B[39m         .squeeze(-\u001B[32m2\u001B[39m)\n\u001B[32m   5621\u001B[39m         .contiguous()\n\u001B[32m   5622\u001B[39m     )\n\u001B[32m   5623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m proj[\u001B[32m0\u001B[39m], proj[\u001B[32m1\u001B[39m], proj[\u001B[32m2\u001B[39m]\n\u001B[32m   5624\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   5625\u001B[39m     \u001B[38;5;66;03m# encoder-decoder attention\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "Unexpected type passed to requires_grad None\n",
      "Attribute \"vertex_normals\" has not been set and failed to be computed due to: 'NoneType' object has no attribute 'unsqueeze'\n",
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "  0%|          | 2/500 [00:00<00:39, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.25363409519195557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 104/500 [00:07<00:26, 14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2665786129236221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 204/500 [00:14<00:19, 15.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.26743394061923026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 304/500 [00:20<00:13, 15.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.26724553436040877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 402/500 [00:27<00:06, 14.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2671678414940834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:33<00:00, 14.78it/s]\n"
     ]
    }
   ],
   "execution_count": 71,
   "source": [
    "sample = val_set[0]\n",
    "\n",
    "mesh, pred_class = optimize_highlighting(\n",
    "    sample=sample,\n",
    "    affordances=affordances,\n",
    "    mlp=mlp,\n",
    "    optimizer=optim,\n",
    "    render=render,\n",
    "    clip_model=clip_model,\n",
    "    tokenizer=tokenizer,\n",
    "    clip_transform=clip_transform,\n",
    "    augment_transform=augment_transform,\n",
    "    n_augs=n_augs,\n",
    "    n_iter=500,\n",
    "    colors=colors,\n",
    "    background=background,\n",
    "    output_dir=output_dir,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
