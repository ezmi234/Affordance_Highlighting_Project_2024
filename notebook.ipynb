{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LL-2kfX9uU9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ed28315-83f2-422e-bce0-a3a7e16c13f2",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:31:07.937864Z",
     "start_time": "2025-05-03T09:31:06.971589Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124, CUDA version: 12.4, GPU available: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1UQHH3b39uU9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "outputId": "f0a9513b-8839-47d3-d207-5f89c82bb5fb",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:31:14.724921Z",
     "start_time": "2025-05-03T09:31:11.944163Z"
    }
   },
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh as mesh\n",
    "import kaolin.ops.conversions as conversions\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "\n",
    "from itertools import permutations, product\n",
    "from kaolin.ops.mesh import face_normals\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import pickle"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warp 1.7.1 initialized:\n",
      "   CUDA Toolkit 12.8, Driver 12.2\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"NVIDIA GeForce RTX 3070 Ti\" (8 GiB, sm_86, mempool enabled)\n",
      "   Kernel cache:\n",
      "     /home/ezmiron/.cache/warp/1.7.1\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# ================== AffordanceNet Dataset HELPER FUNCTIONS =============================\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "      Load the affordance dataset from a pickle file.\n",
    "\n",
    "      Returns a list of dicts with keys:\n",
    "      - 'shape_id'\n",
    "      - 'semantic class'\n",
    "      - 'affordance'\n",
    "      - 'data_info'\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(path, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "        print(\"Loaded train_data\")\n",
    "        for index,info in enumerate(train_data):\n",
    "            temp_info = {}\n",
    "            temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
    "            temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
    "            temp_info[\"affordance\"] = info[\"affordance\"]\n",
    "            temp_info[\"data_info\"] = info[\"full_shape\"]\n",
    "            dataset.append(temp_info)\n",
    "    return dataset\n",
    "\n",
    "def get_coordinates(sample, device='cpu'):\n",
    "    \"\"\"\n",
    "    Returns the point cloud coordinates from a sample as a torch.Tensor on the specified device.\n",
    "\n",
    "    Args:\n",
    "        sample: one entry from the dataset\n",
    "        device: 'cpu' or 'cuda'\n",
    "\n",
    "    Returns:\n",
    "        coords: torch.Tensor of shape [N, 3]\n",
    "    \"\"\"\n",
    "    return torch.tensor(sample[\"data_info\"][\"coordinate\"], dtype=torch.float32).to(device)\n",
    "\n",
    "def get_affordance_classes(sample):\n",
    "    \"\"\"\n",
    "    Returns the list of affordance class names available for the given sample.\n",
    "    \"\"\"\n",
    "    return sample[\"affordance\"]\n",
    "\n",
    "def is_affordance_present(sample, affordance_class):\n",
    "    \"\"\"\n",
    "    Given a sample and an affordance class string (e.g., 'grasp'),\n",
    "    returns True if there is at least one positive label for that affordance,\n",
    "    otherwise False.\n",
    "    \"\"\"\n",
    "    label = np.array(sample[\"data_info\"][\"label\"][affordance_class])\n",
    "    return np.any(label > 0)\n",
    "\n",
    "def get_affordance_label(sample, affordance_class, device='cpu'):\n",
    "    \"\"\"\n",
    "    Returns the binary label mask for a specific affordance class as a torch tensor.\n",
    "\n",
    "    Args:\n",
    "        sample: dataset sample\n",
    "        affordance_class: string key of the affordance label\n",
    "        device: 'cpu' or 'cuda'\n",
    "\n",
    "    Returns:\n",
    "        labels: torch.Tensor of shape [N]\n",
    "    \"\"\"\n",
    "    label = sample[\"data_info\"][\"label\"][affordance_class]\n",
    "    return torch.tensor(label, dtype=torch.float32).squeeze().to(device)"
   ],
   "metadata": {
    "id": "zumHOiMfdH_G",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:32:26.784913Z",
     "start_time": "2025-05-03T09:32:26.780757Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "#Takes a screenshot of the point cloud and saves it to the specified path\n",
    "def render_point_cloud_to_image(pcd, image_path=\"output/tmp_screen.png\"):\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(visible=False)\n",
    "    vis.add_geometry(pcd)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    vis.capture_screen_image(image_path)\n",
    "    vis.destroy_window()\n",
    "\n",
    "\n",
    "#Shows the point cloud in a window or saves it to the specified path\n",
    "def show_point_cloud(point_cloud,render_to_image=False,save_path=\"output/tmp_screen.png\"):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "    if(render_to_image):\n",
    "        render_point_cloud_to_image(pcd,save_path)\n",
    "    else:\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "#Shows the point cloud in a window or saves it to the specified path, coloring the points based on the probabilities\n",
    "def show_point_cloud_tresholded(point_cloud,probs,treshold,render_to_image=False,save_path=\"output/tmp_screen.png\"):\n",
    "\n",
    "    #if probs has 2 columns, delete the second one (refer to the \"no object\" class)\n",
    "    if probs.shape[1]==2:\n",
    "        probs = probs[:,0]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "    colors = np.zeros((len(point_cloud),3))\n",
    "    for i in range(len(point_cloud)):\n",
    "        if probs[i]>treshold:\n",
    "            colors[i] = [1,0,0]\n",
    "        else:\n",
    "            colors[i] = [0.5,0.5,0.5]\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    if render_to_image:\n",
    "        render_point_cloud_to_image(pcd,save_path)\n",
    "    else:\n",
    "        o3d.visualization.draw([pcd])\n",
    "\n",
    "\n",
    "def create_point_cloud_from_mesh(mesh_path,name):\n",
    "    mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "    pcd = mesh.sample_points_uniformly(number_of_points=10000) #Tune if needed.\n",
    "    o3d.io.write_point_cloud(f\"output/{name}.ply\", pcd)\n",
    "    return pcd"
   ],
   "metadata": {
    "id": "TgiO4pKXFaUp",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:32:32.472434Z",
     "start_time": "2025-05-03T09:32:32.465309Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('data/full-shape/full_shape_train_data.pkl')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1p6JpRqFdpy",
    "outputId": "0588ff19-3410-4bb9-a32d-904f3a0f4d50",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:32:53.462488Z",
     "start_time": "2025-05-03T09:32:50.877406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_data\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "sample = dataset[660]"
   ],
   "metadata": {
    "id": "6LU6bFMBGQK2",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:32:54.437681Z",
     "start_time": "2025-05-03T09:32:54.435766Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "print(sample.keys())\n",
    "print(sample[\"semantic class\"])\n",
    "print(get_affordance_classes(sample))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEIK7UtPGQWD",
    "outputId": "9f81b89a-4627-4412-f4b2-ef3cd04bb9f2",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:41:21.245102Z",
     "start_time": "2025-05-03T09:41:21.242922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['shape_id', 'semantic class', 'affordance', 'data_info'])\n",
      "Chair\n",
      "['grasp', 'contain', 'lift', 'openable', 'layable', 'sittable', 'support', 'wrap_grasp', 'pourable', 'move', 'displaY', 'pushable', 'pull', 'listen', 'wear', 'press', 'cut', 'stab']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "pointcloud = get_coordinates(sample).cpu().numpy()"
   ],
   "metadata": {
    "id": "_-DhPRj7IqlO",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:43:35.698218Z",
     "start_time": "2025-05-03T09:43:35.696220Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "# Render and save\n",
    "show_point_cloud(pointcloud)"
   ],
   "metadata": {
    "id": "1EkJCrpDGjCt",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:43:37.980561Z",
     "start_time": "2025-05-03T09:43:36.771058Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EmJVQizM9uU-",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:33:13.543768Z",
     "start_time": "2025-05-03T09:33:13.535007Z"
    }
   },
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "\n",
    "def clip_loss(rendered_images, prompt, clip_model, aug_transform, n_augs, device, tokenizer):\n",
    "    \"\"\"\n",
    "    Computes the CLIP loss as negative cosine similarity between\n",
    "    rendered image embeddings and the text prompt embedding.\n",
    "\n",
    "    Args:\n",
    "        rendered_images (torch.Tensor): shape (B, 3, H, W)\n",
    "        text_prompt (str): e.g., \"a gray chair with highlighted seat\"\n",
    "        clip_model (torch.nn.Module): preloaded CLIP model\n",
    "        device (str): \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        loss (torch.Tensor): scalar CLIP loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode text\n",
    "    text_encoded = tokenizer([prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_encoded)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    for _ in range(n_augs):\n",
    "      aug_image = aug_transform(rendered_images)\n",
    "      image_encoded = clip_model.encode_image(aug_image)\n",
    "      loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
    "\n",
    "    return loss / n_augs\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dluhTREE9uU_",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:33:17.518504Z",
     "start_time": "2025-05-03T09:33:17.514543Z"
    }
   },
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "# ==== Set Seed for Determinism ====\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b0e58601",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:33:19.543790Z",
     "start_time": "2025-05-03T09:33:19.539963Z"
    }
   },
   "source": [
    "def pointcloud_to_voxel_mesh(points, resolution=64, threshold=0.5, export_path=None):\n",
    "  min_coords, _ = points.min(dim=0)\n",
    "  max_coords, _ = points.max(dim=0)\n",
    "  scale = max_coords - min_coords\n",
    "  points_norm = (points - min_coords) / scale\n",
    "\n",
    "  voxel_grid = conversions.pointclouds_to_voxelgrids(points_norm.unsqueeze(0), resolution=resolution).to(device)\n",
    "  verts_faces = conversions.voxelgrids_to_trianglemeshes(voxel_grid, iso_value=threshold)\n",
    "\n",
    "  verts = verts_faces[0][0].cpu() / resolution\n",
    "  faces = verts_faces[1][0].cpu()\n",
    "\n",
    "  # Denormalize\n",
    "  scale = scale.cpu()\n",
    "  min_coords = min_coords.cpu()\n",
    "  verts = verts * scale + min_coords\n",
    "\n",
    "  if verts.numel() == 0 or faces.numel() == 0:\n",
    "      raise ValueError(\"Empty mesh generated from voxel grid.\")\n",
    "\n",
    "  # Create mesh\n",
    "  mesh = trimesh.Trimesh(vertices=verts.numpy(), faces=faces.numpy())\n",
    "\n",
    "  # Smoothing and export\n",
    "  mesh = trimesh.smoothing.filter_laplacian(\n",
    "      mesh, lamb=0.2, iterations=8,\n",
    "      implicit_time_integration=False,\n",
    "      volume_constraint=True\n",
    "  )\n",
    "\n",
    "  if export_path:\n",
    "    mesh.export(export_path)\n",
    "\n",
    "  return mesh"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "def load_vertices(data):\n",
    "  if type(data) == str:\n",
    "    mesh = o3d.io.read_triangle_mesh(data)\n",
    "    mesh.compute_vertex_normals()\n",
    "    vertices = mesh.sample_points_uniformly(number_of_points=4096)\n",
    "    return torch.tensor(np.asarray(vertices.points), dtype=torch.float32).to(device)\n",
    "  else:\n",
    "    return torch.tensor(data, dtype=torch.float32).to(device)"
   ],
   "metadata": {
    "id": "erYMIwWHtrx8",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:33:23.086115Z",
     "start_time": "2025-05-03T09:33:23.082669Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LwpTp2c59uU_",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:33:26.948459Z",
     "start_time": "2025-05-03T09:33:26.946189Z"
    }
   },
   "source": [
    "# ==== Hyperparameters and Settings ====\n",
    "render_res = 224\n",
    "learning_rate = 0.00005\n",
    "n_iter = 2200\n",
    "obj_path = 'data/dog.obj'\n",
    "n_augs = 1\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-B/16'\n",
    "prompt = 'A gray chair with highlighted shoes'"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t9ZWV2gj9uU_",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:34:04.903803Z",
     "start_time": "2025-05-03T09:34:04.900783Z"
    }
   },
   "source": [
    "# ==== Setup Output Directory ====\n",
    "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "log_dir = output_dir"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zBIP-dE69uU_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "974c1919-fc0e-45ce-b920-66abd272ab7c",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:34:08.826818Z",
     "start_time": "2025-05-03T09:34:06.728289Z"
    }
   },
   "source": [
    "# ==== Load Mesh ====\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "\n",
    "# ==== CLIP ====\n",
    "clip_model, preprocess = get_clip_model(clip_model_name)\n",
    "tokenizer = clip.tokenize\n",
    "\n",
    "clip_normalizer = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],std=[0.26862954, 0.26130258, 0.27577711]) #from https://github.com/openai/CLIP/issues/20\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "        clip_normalizer\n",
    "    ])\n",
    "\n",
    "# ==== Neural Highlighter ====\n",
    "mlp = NeuralHighlighter().to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "n_views = 7\n",
    "losses = []"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: ViT-B/16 on cuda (jit=False)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"data/full-shape/full_shape_train_data.pkl\")"
   ],
   "metadata": {
    "id": "MhhPReA0_nWu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a941c3d1-d091-4d35-8434-e1cb12143fc4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded train_data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sample = dataset[3915]"
   ],
   "metadata": {
    "id": "ccnxkuoPOuQi",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:34:22.052178Z",
     "start_time": "2025-05-03T09:34:22.050309Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "vertices = get_coordinates(sample, device)"
   ],
   "metadata": {
    "id": "tzW-B4yFx2Zf",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:34:24.326517Z",
     "start_time": "2025-05-03T09:34:24.324177Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "affordances = get_affordance_classes(sample)\n",
    "\n",
    "for affordance in affordances:\n",
    "    print(affordance, is_affordance_present(sample, affordance))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eim9ZGUhx2f8",
    "outputId": "dfab9485-e792-4fce-c934-66d92a01ff59"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "grasp False\n",
      "contain False\n",
      "lift False\n",
      "openable False\n",
      "layable False\n",
      "sittable True\n",
      "support True\n",
      "wrap_grasp False\n",
      "pourable False\n",
      "move True\n",
      "displaY False\n",
      "pushable False\n",
      "pull False\n",
      "listen False\n",
      "wear False\n",
      "press False\n",
      "cut False\n",
      "stab False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "affordance_label = get_affordance_label(sample, affordance_class='move', device=device)\n",
    "print(affordance_label.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vug5ETBcx2lJ",
    "outputId": "a92ba4f5-c0b2-43c0-e9b3-4093e2f5ce2c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2048])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "mesh = pointcloud_to_voxel_mesh(\n",
    "    vertices,  # sampled point cloud from Open3D\n",
    "    resolution=16,\n",
    "    threshold=0.5,\n",
    "    export_path=\"data/chair_voxel.obj\"\n",
    ")"
   ],
   "metadata": {
    "id": "fy9F4xPv2IAt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sampled_mesh = Mesh(\"data/chair_voxel.obj\")\n",
    "MeshNormalizer(sampled_mesh)()\n",
    "vertices = torch.tensor(sampled_mesh.vertices, dtype=torch.float32, device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jN-_8xh23oB",
    "outputId": "5c3b824b-c050-4a34-9b6c-4866b12dd5e2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:kaolin.rep.surface_mesh:Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "WARNING:kaolin.rep.surface_mesh:Unexpected type passed to requires_grad None\n",
      "WARNING:kaolin.rep.surface_mesh:Attribute \"vertex_normals\" has not been set and failed to be computed due to: 'NoneType' object has no attribute 'unsqueeze'\n",
      "WARNING:kaolin.rep.surface_mesh:Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "<ipython-input-95-a12a955e19e0>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vertices = torch.tensor(sampled_mesh.vertices, dtype=torch.float32, device=device)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Step 1: Original coordinates and labels (on point cloud)\n",
    "orig_coords = get_coordinates(sample, device='cpu')  # [N, 3]\n",
    "orig_labels = get_affordance_label(sample, 'move', device='cpu')  # [N]"
   ],
   "metadata": {
    "id": "TZrmvkxi8vHl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "\n",
    "# Step 2: Voxelized mesh face centers\n",
    "voxel_faces = sampled_mesh.faces  # [F, 3]\n",
    "voxel_vertices = torch.tensor(sampled_mesh.vertices, dtype=torch.float32).clone().detach()\n",
    "face_centers = voxel_vertices[voxel_faces].mean(dim=1)  # [F, 3]\n",
    "\n",
    "# Step 3: KDTree from original coords\n",
    "tree = cKDTree(orig_coords.numpy())\n",
    "_, indices = tree.query(face_centers.cpu().numpy(), k=1)\n",
    "\n",
    "# Step 4: Assign labels from nearest points\n",
    "face_labels = orig_labels[indices]  # shape [F]\n",
    "face_labels = (face_labels > 0.5).long()  # Binarize\n",
    "\n",
    "# Step 5: Convert to one-hot for color_mesh\n",
    "pred_class = torch.nn.functional.one_hot(face_labels, num_classes=2).float().to(device)\n",
    "\n",
    "num_background = int(pred_class[:, 0].sum())\n",
    "num_highlighted = int(pred_class[:, 1].sum())\n",
    "num_highlighted_affordances = int(affordance_label.sum())\n",
    "\n",
    "print(f\"Background points: {num_background}\")\n",
    "print(f\"Highlighted points: {num_highlighted}\")\n",
    "print(f\"Highlighted affordances: {num_highlighted_affordances}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wt70hfTL3Awc",
    "outputId": "5c0b1c94-02b4-4ed3-b2ea-d78c427e88e4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Background points: 2583\n",
      "Highlighted points: 485\n",
      "Highlighted affordances: 416\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-103-4611402e8c7d>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  voxel_vertices = torch.tensor(sampled_mesh.vertices, dtype=torch.float32).clone().detach()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "color_mesh(pred_class, sampled_mesh, colors)\n",
    "mlp = NeuralHighlighter().to(device)\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "save_renders(log_dir, 0, render.render_views(\n",
    "    sampled_mesh,\n",
    "    num_views=5,\n",
    "    show=False,\n",
    "    center_azim=0,\n",
    "    center_elev=0,\n",
    "    std=1,\n",
    "    return_views=True,\n",
    "    lighting=True,\n",
    "    background=background\n",
    ")[0], name='initial_render.jpg')"
   ],
   "metadata": {
    "id": "JwY51KZh5rVk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = np.array(sample[\"data_info\"][\"label\"][sample[\"affordance\"][5]])\n",
    "# vertices = load_vertices(data)\n",
    "temp_obj_path = \"outputDemo.obj\"\n",
    "\n",
    "mesh = pointcloud_to_voxel_mesh(\n",
    "    vertices,  # sampled point cloud from Open3D\n",
    "    resolution=16,\n",
    "    threshold=0.5,\n",
    "    export_path=temp_obj_path\n",
    ")\n",
    "\n",
    "# === Load the voxel mesh from disk ===\n",
    "sampled_mesh = Mesh(temp_obj_path)\n",
    "MeshNormalizer(sampled_mesh)()\n",
    "vertices = torch.tensor(sampled_mesh.vertices, dtype=torch.float32, device=device)"
   ],
   "metadata": {
    "id": "keUDyPoTQqsj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0bd4b878-dfd5-4fbe-953d-ec7af8521df3",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:35:02.976611Z",
     "start_time": "2025-05-03T09:35:02.692565Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezmiron/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/kaolin/ops/conversions/pointcloud.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:644.)\n",
      "  vg = torch.sparse.FloatTensor(\n",
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "Unexpected type passed to requires_grad None\n",
      "Attribute \"vertex_normals\" has not been set and failed to be computed due to: 'NoneType' object has no attribute 'unsqueeze'\n",
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "/tmp/ipykernel_73851/578316982.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vertices = torch.tensor(sampled_mesh.vertices, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a9jR6gD79uVA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8254c837-9916-4b7f-f809-df32eee4de08",
    "ExecuteTime": {
     "end_time": "2025-05-03T09:37:45.341704Z",
     "start_time": "2025-05-03T09:35:17.072611Z"
    }
   },
   "source": [
    "# Optimization loop\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # === Predict highlight probabilities ===\n",
    "    pred_class = mlp(vertices)\n",
    "\n",
    "    # === Color mesh ===\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "\n",
    "    # === Render the mesh ===\n",
    "    rendered_images, elev, azim = render.render_views(\n",
    "        sampled_mesh,\n",
    "        num_views=n_views,\n",
    "        show=False,\n",
    "        center_azim=0,\n",
    "        center_elev=0,\n",
    "        std=1,\n",
    "        return_views=True,\n",
    "        lighting=True,\n",
    "        background=background\n",
    "    )\n",
    "\n",
    "    # === Compute CLIP loss ===\n",
    "    loss = clip_loss(rendered_images, prompt, clip_model, aug_transform, n_augs, device, tokenizer)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optim.step()\n",
    "\n",
    "    # === Save and log results ===\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Last 100 CLIP score: {np.mean(losses[-100:])}\")\n",
    "        save_renders(log_dir, i, rendered_images)\n",
    "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
    "\n",
    "# Remove generated obj\n",
    "os.remove(temp_obj_path)\n",
    "\n",
    "# Final save\n",
    "save_final_results(log_dir, objbase, sampled_mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "# Save prompt\n",
    "with open(os.path.join(output_dir, prompt), \"w\") as f:\n",
    "    f.write('')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2200 [00:00<10:07,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2020263671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 102/2200 [00:07<02:21, 14.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.239324951171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 202/2200 [00:14<02:14, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.251121826171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 302/2200 [00:22<03:02, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.251353759765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 404/2200 [00:30<02:18, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.25096435546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 502/2200 [00:37<01:54, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.250516357421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 604/2200 [00:43<01:47, 14.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.250628662109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 704/2200 [00:50<01:33, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.254281005859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 804/2200 [00:56<01:28, 15.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2530712890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 904/2200 [01:03<01:22, 15.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2530908203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 1004/2200 [01:09<01:15, 15.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.253240966796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1102/2200 [01:15<01:13, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.253057861328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1204/2200 [01:22<01:05, 15.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.253887939453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1304/2200 [01:29<00:56, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2531494140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1404/2200 [01:35<00:50, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.254544677734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1504/2200 [01:42<00:47, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.253458251953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1604/2200 [01:49<00:38, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.25267578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1702/2200 [01:55<00:33, 14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.252388916015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1804/2200 [02:02<00:26, 15.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2532177734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 1902/2200 [02:08<00:19, 14.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.254705810546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 2004/2200 [02:15<00:12, 15.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.25244873046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2102/2200 [02:21<00:06, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.254456787109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [02:28<00:00, 14.85it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset[3915][\"data_info\"].keys())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89g_WE7YOxva",
    "outputId": "4e748330-b27f-4bec-f16e-b1784a95353b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['coordinate', 'label'])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sample = dataset[3915]\n",
    "coords = np.array(sample[\"data_info\"][\"coordinate\"])\n",
    "labels = np.array(sample[\"data_info\"][\"label\"][sample[\"affordance\"][5]])\n",
    "print(labels.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNyJQyeYOe1N",
    "outputId": "9f799bc7-e30b-4b07-b9f3-cc1edc40d192"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2048, 1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "uGjHx15gRHg4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%rm -rf output/"
   ],
   "metadata": {
    "id": "Tc0qhHMrFsg9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pzi10qlPBqKB"
   },
   "outputs": [],
   "source": [
    "%rm -rf output/renders/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
