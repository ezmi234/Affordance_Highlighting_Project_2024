{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LL-2kfX9uU9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1847d7dd-34c8-47ac-8d09-c7203234afe3",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:22.148111Z",
     "start_time": "2025-05-04T14:20:21.215369Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124, CUDA version: 12.4, GPU available: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1UQHH3b39uU9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "outputId": "b6a8e90e-94e8-4855-b3cc-07299f5f1850",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:21:38.733836Z",
     "start_time": "2025-05-04T14:21:38.730891Z"
    }
   },
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh as mesh\n",
    "import kaolin.ops.conversions as conversions\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import permutations, product\n",
    "from kaolin.ops.mesh import face_normals\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import pickle\n",
    "from scipy.spatial import cKDTree\n",
    "from utilities.dataset import load_dataset, get_coordinates, get_affordance_classes, get_affordance_label, is_affordance_present, split_dataset\n",
    "from utilities.point_cloud import pointcloud_to_voxel_mesh, project_vertex_scores_to_pointcloud, visualize_affordance_pointcloud"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:31.254755Z",
     "start_time": "2025-05-04T14:20:31.248518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))  # 2-class output\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "def clip_loss(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # Encode text\n",
    "    text_tokens = tokenizer([text_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        image_features = clip_model.encode_image(clip_image).float()\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        loss = -torch.mean(torch.cosine_similarity(image_features, text_features))\n",
    "\n",
    "    else:\n",
    "        for _ in range(n_augs):\n",
    "          aug_image = aug_transform(rendered_images)\n",
    "          image_encoded = clip_model.encode_image(aug_image)\n",
    "          loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
    "\n",
    "        loss =  loss / n_augs\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dluhTREE9uU_",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:34.414564Z",
     "start_time": "2025-05-04T14:20:34.411504Z"
    }
   },
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "# ==== Set Seed for Determinism ====\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def load_vertices(data):\n",
    "  if type(data) == str:\n",
    "    mesh = o3d.io.read_triangle_mesh(data)\n",
    "    mesh.compute_vertex_normals()\n",
    "    vertices = mesh.sample_points_uniformly(number_of_points=4096)\n",
    "    return torch.tensor(np.asarray(vertices.points), dtype=torch.float32).to(device)\n",
    "  else:\n",
    "    return torch.tensor(data, dtype=torch.float32).to(device)"
   ],
   "metadata": {
    "id": "erYMIwWHtrx8",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:38.916710Z",
     "start_time": "2025-05-04T14:20:38.913168Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LwpTp2c59uU_",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:40.776223Z",
     "start_time": "2025-05-04T14:20:40.774263Z"
    }
   },
   "source": [
    "# ==== Hyperparameters and Settings ====\n",
    "render_res = 224\n",
    "learning_rate = 0.00005\n",
    "n_iter = 2200\n",
    "n_augs = 1\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-B/16'"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zBIP-dE69uU_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cc4a00be-3a16-4354-96fe-51b1c9781986",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:46.303028Z",
     "start_time": "2025-05-04T14:20:44.170360Z"
    }
   },
   "source": [
    "# ==== Device ====\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "\n",
    "# ==== CLIP ====\n",
    "clip_model, preprocess = get_clip_model(clip_model_name)\n",
    "tokenizer = clip.tokenize\n",
    "\n",
    "# ==== Normalization and Augmentation ====\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((render_res, render_res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "n_views = 7\n",
    "losses = []"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: ViT-B/16 on cuda (jit=False)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "original_dataset = load_dataset(\"data/full-shape/full_shape_train_data.pkl\")"
   ],
   "metadata": {
    "id": "MhhPReA0_nWu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b03b7e7f-63db-4f2e-d213-3b9e399f051b",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:50.866638Z",
     "start_time": "2025-05-04T14:20:48.271953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_data\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "affordances = ['grasp', 'wrap grasp', 'pull']\n",
    "\n",
    "val_set, test_set = split_dataset(original_dataset, val_ratio=0.01, seed=42)\n",
    "print(f\"Validation set size: {len(val_set)}, Test set size: {len(test_set)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JYk66K3WoDP",
    "outputId": "9951d853-a62a-429d-a3c5-11099600c175",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:52.233210Z",
     "start_time": "2025-05-04T14:20:52.226493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 160, Test set size: 160\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:20:53.715749Z",
     "start_time": "2025-05-04T14:20:53.711579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_counts = {}\n",
    "test_counts = {}\n",
    "\n",
    "for item in val_set:\n",
    "    cls = item[\"semantic class\"]\n",
    "    if cls not in val_counts:\n",
    "        val_counts[cls] = 0\n",
    "    val_counts[cls] += 1\n",
    "\n",
    "for item in test_set:\n",
    "    cls = item[\"semantic class\"]\n",
    "    if cls not in test_counts:\n",
    "        test_counts[cls] = 0\n",
    "    test_counts[cls] += 1\n",
    "\n",
    "sorted_val = sorted(val_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_test = sorted(test_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Validation Set Semantic Class Counts:\")\n",
    "for cls, count in sorted_val:\n",
    "    print(f\"{cls}: {count}\")\n",
    "\n",
    "print(\"\\nTest Set Semantic Class Counts:\")\n",
    "for cls, count in sorted_test:\n",
    "    print(f\"{cls}: {count}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Semantic Class Counts:\n",
      "Table: 52\n",
      "Chair: 39\n",
      "Vase: 14\n",
      "Bottle: 10\n",
      "StorageFurniture: 7\n",
      "Refrigerator: 6\n",
      "Faucet: 4\n",
      "TrashCan: 4\n",
      "Display: 3\n",
      "Door: 3\n",
      "Knife: 3\n",
      "Bed: 3\n",
      "Clock: 3\n",
      "Dishwasher: 2\n",
      "Hat: 2\n",
      "Keyboard: 1\n",
      "Bowl: 1\n",
      "Mug: 1\n",
      "Laptop: 1\n",
      "Earphone: 1\n",
      "\n",
      "Test Set Semantic Class Counts:\n",
      "Table: 62\n",
      "Chair: 41\n",
      "StorageFurniture: 18\n",
      "Display: 6\n",
      "Clock: 5\n",
      "Vase: 4\n",
      "TrashCan: 3\n",
      "Laptop: 3\n",
      "Refrigerator: 2\n",
      "Microwave: 2\n",
      "Faucet: 2\n",
      "Mug: 2\n",
      "Dishwasher: 2\n",
      "Keyboard: 1\n",
      "Bed: 1\n",
      "Bag: 1\n",
      "Hat: 1\n",
      "Door: 1\n",
      "Scissors: 1\n",
      "Bowl: 1\n",
      "Knife: 1\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:21:01.731083Z",
     "start_time": "2025-05-04T14:21:01.728720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_prompt(semantic_class, affordance):\n",
    "  temp_prompt = \"a gray \" + str(semantic_class).lower() + \" with highlighted \" + str(affordance).lower() + \" region\"\n",
    "  return temp_prompt"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:21:42.731224Z",
     "start_time": "2025-05-04T14:21:42.727838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_vertex_scores(pred_class: torch.Tensor, positive_class: int = 1):\n",
    "    \"\"\"\n",
    "    Returns vertex-wise confidence scores for the positive class.\n",
    "\n",
    "    Args:\n",
    "        pred_class (torch.Tensor): shape [N, 2], softmax logits\n",
    "        positive_class (int): which class index should be interpreted as affordance (1 or 0)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape [N], probabilities\n",
    "    \"\"\"\n",
    "    probs = F.softmax(pred_class, dim=1)\n",
    "    return probs[:, positive_class]\n",
    "\n",
    "def compute_mIoU(pred_labels: torch.Tensor, gt_labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes binary mean Intersection over Union.\n",
    "\n",
    "    Args:\n",
    "        pred_labels (torch.Tensor): shape [N], binary 0/1\n",
    "        gt_labels (torch.Tensor): shape [N], binary 0/1\n",
    "\n",
    "    Returns:\n",
    "        float: IoU score\n",
    "    \"\"\"\n",
    "    pred = pred_labels.bool()\n",
    "    gt = gt_labels.bool()\n",
    "    intersection = (pred & gt).sum().float()\n",
    "    union = (pred | gt).sum().float()\n",
    "    return (intersection / union).item() if union > 0 else float('nan')\n",
    "\n",
    "def optimize_mIoU_threshold(projected_scores, gt_labels, thresholds=None, gt_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Computes IoU over different thresholds on predicted scores.\n",
    "\n",
    "    Args:\n",
    "        projected_scores (torch.Tensor): shape [N], soft prediction per point\n",
    "        gt_labels (torch.Tensor): shape [N], soft or binary GT labels\n",
    "        thresholds (list or tensor): list of thresholds to test (default: 0.1 to 0.9)\n",
    "        gt_threshold (float): threshold to binarize ground truth\n",
    "\n",
    "    Returns:\n",
    "        (float, float): best threshold, best IoU\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = torch.linspace(0.1, 0.9, steps=9)\n",
    "\n",
    "    gt_binary = (gt_labels > gt_threshold).long()\n",
    "\n",
    "    best_iou = -1\n",
    "    best_thresh = 0.5\n",
    "\n",
    "    for t in thresholds:\n",
    "        pred_binary = (projected_scores > t).long()\n",
    "        iou = compute_mIoU(pred_binary, gt_binary)\n",
    "        print(f\"Threshold {t:.2f} â†’ IoU: {iou:.4f}\")\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_thresh = float(t)\n",
    "\n",
    "    return best_thresh, best_iou\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:21:44.116280Z",
     "start_time": "2025-05-04T14:21:44.108555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_highlighting(sample, affordance, mlp, optimizer, render, clip_model, tokenizer, clip_transform, augment_transform, n_augs, n_iter, colors, background, output_dir, device):\n",
    "    \"\"\"\n",
    "    Optimizes the highlighting process for a given sample and affordances.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): The input sample containing mesh and semantic class information.\n",
    "        affordance: Affordance to be highlighted.\n",
    "        mlp (nn.Module): Neural network model for highlighting.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training the model.\n",
    "        render (Renderer): Renderer for generating views of the mesh.\n",
    "        clip_model (nn.Module): CLIP model for computing loss.\n",
    "        tokenizer (function): Tokenizer for text prompts.\n",
    "        clip_transform (transforms.Compose): Transformations for CLIP input images.\n",
    "        augment_transform (transforms.Compose): Augmentation transformations for images.\n",
    "        n_augs (int): Number of augmentations for CLIP loss.        n_iter (int): Number of optimization iterations.\n",
    "        colors (torch.Tensor): Tensor of colors for highlighting.\n",
    "        background (torch.Tensor): Background color tensor.\n",
    "        output_dir (str): Directory to save results.\n",
    "        device (torch.device): Device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicted class probabilities for vertices.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    export_path = os.path.join(output_dir, f\"run_{timestamp}\")\n",
    "    os.makedirs(os.path.join(export_path, \"renders\"), exist_ok=True)\n",
    "    temp_obj_path = \"data/outputDemo.obj\"\n",
    "\n",
    "    if \"semantic class\" not in sample or not isinstance(sample[\"semantic class\"], str):\n",
    "        raise ValueError(f\"Error: Missing or invalid 'semantic class' field in sample: {sample}\")\n",
    "\n",
    "    prompt = build_prompt(sample[\"semantic class\"], affordance)\n",
    "\n",
    "    points = get_coordinates(sample, device)\n",
    "    trimesh_mesh = pointcloud_to_voxel_mesh(\n",
    "        points,  # sampled point cloud from Open3D\n",
    "        resolution=16,\n",
    "        threshold=0.5,\n",
    "        export_path=temp_obj_path\n",
    "    )\n",
    "\n",
    "\n",
    "    sampled_mesh = Mesh(temp_obj_path)\n",
    "    MeshNormalizer(sampled_mesh)()\n",
    "    vertices = sampled_mesh.vertices.clone().detach().to(device).float()\n",
    "\n",
    "    losses = []\n",
    "    pred_class = None\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        optimizer.zero_grad()\n",
    "        pred_class = mlp(vertices)  # Predict highlight probabilities\n",
    "        color_mesh(pred_class, sampled_mesh, colors)  # Color mesh\n",
    "\n",
    "        # Render and compute loss\n",
    "        rendered_images, elev, azim = render.render_views(\n",
    "            sampled_mesh,\n",
    "            num_views=n_views,\n",
    "            show=False,\n",
    "            center_azim=0,\n",
    "            center_elev=0,\n",
    "            std=1,\n",
    "            return_views=True,\n",
    "            lighting=True,\n",
    "            background=background\n",
    "        )\n",
    "        loss = clip_loss(rendered_images, prompt, clip_transform, clip_model, tokenizer, device, augment_transform, n_augs)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Save the loss for logging\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Log and save intermediate results\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Last 100 CLIP score: {np.mean(losses[-100:])}\")\n",
    "            save_renders(export_path, i, rendered_images)\n",
    "            with open(os.path.join(export_path, \"training_info.txt\"), \"a\") as f:\n",
    "                f.write(f\"Iter {i}: Prompt: {prompt}, Avg CLIP score: {np.mean(losses[-100:])}, CLIP score: {loss.item()}\\n\")\n",
    "\n",
    "    # Final save and cleanup\n",
    "    save_final_results(export_path, sample[\"semantic class\"], sampled_mesh, mlp, vertices, colors, render, background)\n",
    "    with open(os.path.join(export_path, \"prompt.txt\"), \"w\") as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "    if os.path.exists(temp_obj_path):\n",
    "        os.remove(temp_obj_path)\n",
    "\n",
    "    return trimesh_mesh, pred_class, export_path"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for sample in val_set:\n",
    "    # ==== Neural Highlighter ====\n",
    "    mlp = NeuralHighlighter().to(device)\n",
    "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "    print(f\"Training Neural Highlighter for {sample['semantic class']}\")\n",
    "    affordances = []\n",
    "\n",
    "    for elem in get_affordance_classes(sample):\n",
    "        if is_affordance_present(sample, elem):\n",
    "            affordances.append(elem)\n",
    "\n",
    "    if len(affordances) == 0:\n",
    "        print(f\"Warning: No affordances found for sample {sample['semantic class']}\")\n",
    "        continue\n",
    "\n",
    "    random.seed(seed)\n",
    "    affordance = random.choice(affordances)\n",
    "    print(f\"Selected affordance: {affordance} of {affordances}\")\n",
    "\n",
    "\n",
    "    trimesh, pred_class, export_path = optimize_highlighting(\n",
    "        sample=sample,\n",
    "        affordance=affordance,\n",
    "        mlp=mlp,\n",
    "        optimizer=optim,\n",
    "        render=render,\n",
    "        clip_model=clip_model,\n",
    "        tokenizer=tokenizer,\n",
    "        clip_transform=clip_transform,\n",
    "        augment_transform=augment_transform,\n",
    "        n_augs=n_augs,\n",
    "        n_iter=n_iter,\n",
    "        colors=colors,\n",
    "        background=background,\n",
    "        output_dir=output_dir,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Project vertex scores to point cloud\n",
    "    pointcloud = get_coordinates(sample, device)\n",
    "    gt_labels = get_affordance_label(sample, affordance, device)\n",
    "    vertex_scores = get_vertex_scores(pred_class, positive_class=0)  # class 0 = affordance\n",
    "    projected_scores = project_vertex_scores_to_pointcloud(trimesh, vertex_scores, pointcloud, device)\n",
    "\n",
    "    # Optimize IoU threshold\n",
    "    best_thresh, best_iou = optimize_mIoU_threshold(projected_scores, gt_labels)\n",
    "    print(f\"Best IoU threshold: {best_thresh}, Best IoU: {best_iou}\")\n",
    "\n",
    "    # Save results\n",
    "    with open(os.path.join(export_path, \"IoU.txt\"), \"a\") as f:\n",
    "        f.write(f\"Best IoU threshold: {best_thresh}, Best IoU: {best_iou}\\n\")\n",
    "    # Save projected scores\n",
    "    with open(os.path.join(export_path, \"projected_scores.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(projected_scores.detach().cpu().numpy(), f)\n",
    "    # Save ground truth labels\n",
    "    with open(os.path.join(export_path, \"gt_labels.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(gt_labels.detach().cpu().numpy(), f)\n",
    "    # Save point cloud\n",
    "    with open(os.path.join(export_path, \"pointcloud.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(pointcloud.detach().cpu().numpy(), f)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:22:08.850140Z",
     "start_time": "2025-05-04T14:22:05.413817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pointcloud = get_coordinates(val_set[0], device)\n",
    "affordance = 'support'\n",
    "gt_labels = get_affordance_label(val_set[0], affordance, device)\n",
    "visualize_affordance_pointcloud(pointcloud.cpu().numpy(), gt_labels.detach().cpu().numpy(), point_size=8.0)"
   ],
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
