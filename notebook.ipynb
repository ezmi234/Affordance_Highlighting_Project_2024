{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "94Z1Mc-Cb7gx"
   },
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
    "\n",
    "# Downgrade numpy to a compatible version\n",
    "!pip install numpy==1.23.5 --force-reinstall"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "-oBE-NTU88VF"
   },
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/ezmi234/Affordance_Highlighting_Project_2024.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%cd Affordance_Highlighting_Project_2024\n",
    "!git checkout part1-mesh-highlighter"
   ],
   "metadata": {
    "id": "elZUKZwE8-5E"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IXU6wCaDWR0K"
   },
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4ozrcjricJCs"
   },
   "source": [
    "import clip\n",
    "import copy\n",
    "import kaolin as kal\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tPJnFsqzcr79"
   },
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))  # 2-class output\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "def clip_loss(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0):\n",
    "    \"\"\"\n",
    "    Computes the CLIP loss as negative cosine similarity between\n",
    "    rendered image embeddings and the text prompt embedding.\n",
    "\n",
    "    Args:\n",
    "        rendered_images (torch.Tensor): shape (B, 3, H, W)\n",
    "        text_prompt (str): e.g., \"a gray chair with highlighted seat\"\n",
    "        clip_transform (torchvision.transforms): preprocessing for CLIP\n",
    "        clip_model (torch.nn.Module): preloaded CLIP model\n",
    "        tokenizer (callable): CLIP tokenizer\n",
    "        device (str): \"cuda\" or \"cpu\"\n",
    "        aug_transform (torchvision.transforms): augmentation for CLIP\n",
    "        n_augs (int): number of augmentations to apply\n",
    "    Returns:\n",
    "        loss (torch.Tensor): scalar CLIP loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # Encode text\n",
    "    text_tokens = tokenizer([text_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        image_features = clip_model.encode_image(clip_image).float()\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        loss = -torch.mean(torch.cosine_similarity(image_features, text_features))\n",
    "\n",
    "    else:\n",
    "        for _ in range(n_augs):\n",
    "          aug_image = aug_transform(rendered_images)\n",
    "          image_encoded = clip_model.encode_image(aug_image)\n",
    "          loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
    "\n",
    "        loss =  loss / n_augs\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "# ==== Set Seed for Determinism ====\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "id": "ALRtI_fpWWls"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==== Hyperparameters and Settings ====\n",
    "render_res = 224\n",
    "learning_rate = 0.00005\n",
    "n_iter = 2200\n",
    "obj_path = 'data/dog.obj'\n",
    "n_augs = 1\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-B/16'\n",
    "prompt = 'A gray dog with highlighted belt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "S3stycs1WmMO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==== Load Mesh ====\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# ==== CLIP ====\n",
    "clip_model, preprocess = get_clip_model(clip_model_name)\n",
    "tokenizer = clip.tokenize\n",
    "\n",
    "# ==== Normalization and Augmentation ====\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((render_res, render_res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# ==== Neural Highlighter ====\n",
    "mlp = NeuralHighlighter().to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "vertices = copy.deepcopy(mesh.vertices).to(device)\n",
    "n_views = 7\n",
    "losses = []\n"
   ],
   "metadata": {
    "id": "duDsGJbVW3SK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6eb-styAc5AG"
   },
   "source": [
    "# ==== Setup Output Directory ====\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d__%H:%M:%S\")\n",
    "\n",
    "base_path = '/content/drive/MyDrive/affordance_outputs'\n",
    "export_path = os.path.join(base_path, f\"run_{timestamp}\")\n",
    "Path(os.path.join(export_path, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "# ==== Training Loop ====\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # predict highlight probabilities\n",
    "    pred_class = mlp(vertices)\n",
    "\n",
    "    # color and render mesh\n",
    "    sampled_mesh = mesh\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "    rendered_images, elev, azim = render.render_views(sampled_mesh,\n",
    "                                                      num_views=n_views,\n",
    "                                                      show=False,\n",
    "                                                      center_azim=0,\n",
    "                                                      center_elev=0,\n",
    "                                                      std=1,\n",
    "                                                      return_views=True,\n",
    "                                                      lighting=True,\n",
    "                                                      background=background)\n",
    "\n",
    "    # compute CLIP loss\n",
    "    loss = clip_loss(rendered_images, prompt, clip_transform, clip_model, tokenizer, device, augment_transform, n_augs)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optim.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # report\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iter {i} | Last 100 CLIP score: {np.mean(losses[-100:])} | Current loss: {loss.item()}\")\n",
    "        save_renders(export_path, i, rendered_images)\n",
    "        with open(os.path.join(export_path, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"Iter {i} | Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:]):.4f}, Current loss: {loss.item():.4f}\\n\")\n",
    "\n",
    "# ==== Save Final Results ====\n",
    "save_final_results(export_path, objbase, mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "minutes, seconds = divmod(total_time, 60)\n",
    "\n",
    "# ==== Save Prompt ====\n",
    "with open(os.path.join(export_path, \"summary.txt\"), \"w\") as f:\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"CLIP model: {clip_model_name}\\n\")\n",
    "    f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    f.write(f\"Number of iterations: {n_iter}\\n\")\n",
    "    f.write(f\"Number of views: {n_views}\\n\")\n",
    "    f.write(f\"Number of augmentations: {n_augs}\\n\")\n",
    "    f.write(f\"Final CLIP score: {np.mean(losses[-100:]):.4f}\\n\")\n",
    "    f.write(f\"Final loss: {loss.item():.4f}\\n\")\n",
    "    f.write(f\"Total time: {int(minutes)}m {int(seconds)}s\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
