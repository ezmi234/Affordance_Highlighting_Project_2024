{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
    "!pip install trimesh\n",
    "!pip install open3d\n",
    "\n",
    "# Downgrade numpy to a compatible version\n",
    "!pip install numpy==1.23.5 --force-reinstall"
   ],
   "metadata": {
    "id": "FZ1Eoj99pX4b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/ezmi234/Affordance_Highlighting_Project_2024.git"
   ],
   "metadata": {
    "id": "EqtNSSDLqb0n"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%cd Affordance_Highlighting_Project_2024\n",
    "!git checkout part2-pointcloud-adaptation"
   ],
   "metadata": {
    "id": "3UIrzZVzqfwx"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LL-2kfX9uU9"
   },
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1UQHH3b39uU9"
   },
   "source": [
    "import clip\n",
    "import kaolin as kal\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from kaolin.ops.mesh import face_normals\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import pickle\n",
    "from utilities.point_cloud import pointcloud_to_voxel_mesh\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "Z8pejR3WpVwe"
   },
   "cell_type": "code",
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))  # 2-class output\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "def clip_loss(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0):\n",
    "    \"\"\"\n",
    "    Computes the CLIP loss as negative cosine similarity between\n",
    "    rendered image embeddings and the text prompt embedding.\n",
    "\n",
    "    Args:\n",
    "        rendered_images (torch.Tensor): shape (B, 3, H, W)\n",
    "        text_prompt (str): e.g., \"a gray chair with highlighted seat\"\n",
    "        clip_transform (torchvision.transforms): preprocessing for CLIP\n",
    "        clip_model (torch.nn.Module): preloaded CLIP model\n",
    "        tokenizer (callable): CLIP tokenizer\n",
    "        device (str): \"cuda\" or \"cpu\"\n",
    "        aug_transform (torchvision.transforms): augmentation for CLIP\n",
    "        n_augs (int): number of augmentations to apply\n",
    "    Returns:\n",
    "        loss (torch.Tensor): scalar CLIP loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # Encode text\n",
    "    text_tokens = tokenizer([text_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        image_features = clip_model.encode_image(clip_image).float()\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        loss = -torch.mean(torch.cosine_similarity(image_features, text_features))\n",
    "\n",
    "    else:\n",
    "        for _ in range(n_augs):\n",
    "          aug_image = aug_transform(rendered_images)\n",
    "          image_encoded = clip_model.encode_image(aug_image)\n",
    "          loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
    "\n",
    "        loss =  loss / n_augs\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def get_sample(object_class=\"Knife\", seed=42):\n",
    "  path = \"data/full-shape/full_shape_train_data.pkl\"\n",
    "  if not os.path.exists(path):\n",
    "    print(\"Local dataset not found. Downloading...\")\n",
    "    !gdown 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF --output full-shape.zip\n",
    "    !unzip -q full-shape.zip -d data/full-shape\n",
    "  else:\n",
    "      print(\"Local dataset found. Skipping download.\")\n",
    "\n",
    "  # Load and split\n",
    "  dataset = []\n",
    "  with open(path, 'rb') as f:\n",
    "      train_data = pickle.load(f)\n",
    "      print(\"Loaded sample\")\n",
    "      for index,info in enumerate(train_data):\n",
    "          temp_info = {}\n",
    "          temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
    "          temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
    "          temp_info[\"affordance\"] = info[\"affordance\"]\n",
    "          temp_info[\"data_info\"] = info[\"full_shape\"]\n",
    "          dataset.append(temp_info)\n",
    "  filtered_dataset = [item for item in dataset if item['semantic class']]\n",
    "\n",
    "  random.seed(seed)\n",
    "  random.shuffle(filtered_dataset)\n",
    "\n",
    "  sample = filtered_dataset[0]\n",
    "\n",
    "  return sample"
   ],
   "metadata": {
    "id": "zkSmhbWArOBw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "semantic_class = \"Table\"\n",
    "affordance = \"support\""
   ],
   "metadata": {
    "id": "M_LiOfDM5blR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sample = get_sample(semantic_class)\n",
    "print(sample[\"semantic class\"])"
   ],
   "metadata": {
    "id": "lpz1G-qXtAGK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "u_U3aDWg0wuh"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "# ==== Set Seed for Determinism ====\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "metadata": {
    "id": "Fo97TJ8J0wuh"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==== Hyperparameters and Settings ====\n",
    "render_res = 224\n",
    "learning_rate = 0.00005\n",
    "n_iter = 2200\n",
    "temp_obj_path = \"data/temp.obj\"  # Temporary path for the sampled mesh from point cloud\n",
    "n_augs = 1\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-B/16'\n",
    "prompt = f\"A gray {str(semantic_class).lower()} with highlighted {affordance}\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "metadata": {
    "id": "TvTpW66W0wuh"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==== Load Mesh ====\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "\n",
    "points =  torch.tensor(sample[\"data_info\"][\"coordinate\"], dtype=torch.float32).to(device)\n",
    "trimesh_mesh = pointcloud_to_voxel_mesh(\n",
    "    points,\n",
    "    resolution=16,\n",
    "    threshold=0.5,\n",
    "    export_path=temp_obj_path,\n",
    ")\n",
    "\n",
    "mesh = Mesh(temp_obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "vertices = mesh.vertices.clone().detach().to(device).float()\n",
    "\n",
    "# ==== CLIP ====\n",
    "clip_model, preprocess = get_clip_model(clip_model_name)\n",
    "tokenizer = clip.tokenize\n",
    "\n",
    "# ==== Normalization and Augmentation ====\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((render_res, render_res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# ==== Neural Highlighter ====\n",
    "mlp = NeuralHighlighter().to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "n_views = 7\n",
    "losses = []"
   ]
  },
  {
   "metadata": {
    "id": "HBR_1SCppVwg"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==== Setup Output Directory ====\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d__%H:%M:%S\")\n",
    "\n",
    "base_path = '/content/drive/MyDrive/affordance_outputs_on_pointcloud'\n",
    "export_path = os.path.join(base_path, f\"run_{timestamp}\")\n",
    "Path(os.path.join(export_path, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "# ==== Training Loop ====\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # predict highlight probabilities\n",
    "    pred_class = mlp(vertices)\n",
    "\n",
    "    # color and render mesh\n",
    "    sampled_mesh = mesh\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "    rendered_images, elev, azim = render.render_views(sampled_mesh,\n",
    "                                                      num_views=n_views,\n",
    "                                                      show=False,\n",
    "                                                      center_azim=0,\n",
    "                                                      center_elev=0,\n",
    "                                                      std=1,\n",
    "                                                      return_views=True,\n",
    "                                                      lighting=True,\n",
    "                                                      background=background)\n",
    "\n",
    "    # compute CLIP loss\n",
    "    loss = clip_loss(rendered_images, prompt, clip_transform, clip_model, tokenizer, device, augment_transform, n_augs)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optim.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # report\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iter {i} | Last 100 CLIP score: {np.mean(losses[-100:])} | Current loss: {loss.item()}\")\n",
    "        save_renders(export_path, i, rendered_images)\n",
    "        with open(os.path.join(export_path, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"Iter {i} | Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:]):.4f}, Current loss: {loss.item():.4f}\\n\")\n",
    "\n",
    "# ==== Save Final Results ====\n",
    "save_final_results(export_path, semantic_class, mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "minutes, seconds = divmod(total_time, 60)\n",
    "\n",
    "# ==== Save Prompt ====\n",
    "with open(os.path.join(export_path, \"summary.txt\"), \"w\") as f:\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"CLIP model: {clip_model_name}\\n\")\n",
    "    f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    f.write(f\"Number of iterations: {n_iter}\\n\")\n",
    "    f.write(f\"Number of views: {n_views}\\n\")\n",
    "    f.write(f\"Number of augmentations: {n_augs}\\n\")\n",
    "    f.write(f\"Final CLIP score: {np.mean(losses[-100:]):.4f}\\n\")\n",
    "    f.write(f\"Final loss: {loss.item():.4f}\\n\")\n",
    "    f.write(f\"Total time: {int(minutes)}m {int(seconds)}s\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
