{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LL-2kfX9uU9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1847d7dd-34c8-47ac-8d09-c7203234afe3",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:27.780022Z",
     "start_time": "2025-05-04T14:59:27.777425Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Show details\n",
    "print(f\"PyTorch version: {torch.__version__}, CUDA version: {torch.version.cuda}, GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124, CUDA version: 12.4, GPU available: True\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1UQHH3b39uU9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "outputId": "b6a8e90e-94e8-4855-b3cc-07299f5f1850",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:31.157858Z",
     "start_time": "2025-05-04T14:59:30.180485Z"
    }
   },
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh as mesh\n",
    "import kaolin.ops.conversions as conversions\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import permutations, product\n",
    "from kaolin.ops.mesh import face_normals\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import color_mesh\n",
    "import pickle\n",
    "from scipy.spatial import cKDTree\n",
    "from utilities.dataset import load_dataset, get_coordinates, get_affordance_classes, get_affordance_label, is_affordance_present, split_dataset\n",
    "from utilities.point_cloud import pointcloud_to_voxel_mesh, project_vertex_scores_to_pointcloud, visualize_affordance_pointcloud"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:32.437240Z",
     "start_time": "2025-05-04T14:59:32.431041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, output_dim=2, num_layers=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: usually 3 (x, y, z)\n",
    "            hidden_dim: size of hidden layers\n",
    "            output_dim: 2 for [highlight, gray]\n",
    "            num_layers: total number of linear layers\n",
    "        \"\"\"\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.LayerNorm(hidden_dim)]\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))  # 2-class output\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def get_clip_model(clipmodel='ViT-L/14', jit=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(clipmodel, device=device, jit=jit)\n",
    "    print(f\"Loaded CLIP model: {clipmodel} on {device} (jit={jit})\")\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "def clip_loss(rendered_images, text_prompt, clip_transform, clip_model, tokenizer, device, aug_transform=None, n_augs=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # Encode text\n",
    "    text_tokens = tokenizer([text_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # L2 norm\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        image_features = clip_model.encode_image(clip_image).float()\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        loss = -torch.mean(torch.cosine_similarity(image_features, text_features))\n",
    "\n",
    "    else:\n",
    "        for _ in range(n_augs):\n",
    "          aug_image = aug_transform(rendered_images)\n",
    "          image_encoded = clip_model.encode_image(aug_image)\n",
    "          loss -= torch.mean(torch.cosine_similarity(image_encoded, text_features))\n",
    "\n",
    "        loss =  loss / n_augs\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dluhTREE9uU_",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:36.714282Z",
     "start_time": "2025-05-04T14:59:36.711218Z"
    }
   },
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "\n",
    "# ==== Set Seed for Determinism ====\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "def load_vertices(data):\n",
    "  if type(data) == str:\n",
    "    mesh = o3d.io.read_triangle_mesh(data)\n",
    "    mesh.compute_vertex_normals()\n",
    "    vertices = mesh.sample_points_uniformly(number_of_points=4096)\n",
    "    return torch.tensor(np.asarray(vertices.points), dtype=torch.float32).to(device)\n",
    "  else:\n",
    "    return torch.tensor(data, dtype=torch.float32).to(device)"
   ],
   "metadata": {
    "id": "erYMIwWHtrx8",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:39.545269Z",
     "start_time": "2025-05-04T14:59:39.541147Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LwpTp2c59uU_",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:42.495426Z",
     "start_time": "2025-05-04T14:59:42.493137Z"
    }
   },
   "source": [
    "# ==== Hyperparameters and Settings ====\n",
    "render_res = 224\n",
    "learning_rate = 0.00005\n",
    "n_iter = 2200\n",
    "n_augs = 1\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-B/16'"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zBIP-dE69uU_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cc4a00be-3a16-4354-96fe-51b1c9781986",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:45.593696Z",
     "start_time": "2025-05-04T14:59:43.741965Z"
    }
   },
   "source": [
    "# ==== Device ====\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "\n",
    "# ==== CLIP ====\n",
    "clip_model, preprocess = get_clip_model(clip_model_name)\n",
    "tokenizer = clip.tokenize\n",
    "\n",
    "# ==== Normalization and Augmentation ====\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((render_res, render_res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(render_res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# ==== Colors and Other Constants ====\n",
    "colors = torch.tensor([[204/255, 1., 0.], [180/255, 180/255, 180/255]]).to(device)\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "n_views = 7\n",
    "losses = []"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: ViT-B/16 on cuda (jit=False)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "original_dataset = load_dataset(\"data/full-shape/full_shape_train_data.pkl\")"
   ],
   "metadata": {
    "id": "MhhPReA0_nWu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b03b7e7f-63db-4f2e-d213-3b9e399f051b",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:49.305414Z",
     "start_time": "2025-05-04T14:59:47.197760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_data\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "affordances = ['grasp', 'wrap grasp', 'pull']\n",
    "\n",
    "val_set, test_set = split_dataset(original_dataset, val_ratio=0.01, seed=42)\n",
    "print(f\"Validation set size: {len(val_set)}, Test set size: {len(test_set)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JYk66K3WoDP",
    "outputId": "9951d853-a62a-429d-a3c5-11099600c175",
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:49.738228Z",
     "start_time": "2025-05-04T14:59:49.733146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 160, Test set size: 160\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:59:51.025753Z",
     "start_time": "2025-05-04T14:59:51.022098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_counts = {}\n",
    "test_counts = {}\n",
    "\n",
    "for item in val_set:\n",
    "    cls = item[\"semantic class\"]\n",
    "    if cls not in val_counts:\n",
    "        val_counts[cls] = 0\n",
    "    val_counts[cls] += 1\n",
    "\n",
    "for item in test_set:\n",
    "    cls = item[\"semantic class\"]\n",
    "    if cls not in test_counts:\n",
    "        test_counts[cls] = 0\n",
    "    test_counts[cls] += 1\n",
    "\n",
    "sorted_val = sorted(val_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_test = sorted(test_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Validation Set Semantic Class Counts:\")\n",
    "for cls, count in sorted_val:\n",
    "    print(f\"{cls}: {count}\")\n",
    "\n",
    "print(\"\\nTest Set Semantic Class Counts:\")\n",
    "for cls, count in sorted_test:\n",
    "    print(f\"{cls}: {count}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Semantic Class Counts:\n",
      "Table: 52\n",
      "Chair: 39\n",
      "Vase: 14\n",
      "Bottle: 10\n",
      "StorageFurniture: 7\n",
      "Refrigerator: 6\n",
      "Faucet: 4\n",
      "TrashCan: 4\n",
      "Display: 3\n",
      "Door: 3\n",
      "Knife: 3\n",
      "Bed: 3\n",
      "Clock: 3\n",
      "Dishwasher: 2\n",
      "Hat: 2\n",
      "Keyboard: 1\n",
      "Bowl: 1\n",
      "Mug: 1\n",
      "Laptop: 1\n",
      "Earphone: 1\n",
      "\n",
      "Test Set Semantic Class Counts:\n",
      "Table: 62\n",
      "Chair: 41\n",
      "StorageFurniture: 18\n",
      "Display: 6\n",
      "Clock: 5\n",
      "Vase: 4\n",
      "TrashCan: 3\n",
      "Laptop: 3\n",
      "Refrigerator: 2\n",
      "Microwave: 2\n",
      "Faucet: 2\n",
      "Mug: 2\n",
      "Dishwasher: 2\n",
      "Keyboard: 1\n",
      "Bed: 1\n",
      "Bag: 1\n",
      "Hat: 1\n",
      "Door: 1\n",
      "Scissors: 1\n",
      "Bowl: 1\n",
      "Knife: 1\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:00:02.089193Z",
     "start_time": "2025-05-04T15:00:02.087279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_prompt(semantic_class, affordance):\n",
    "  temp_prompt = \"a gray \" + str(semantic_class).lower() + \" with highlighted \" + str(affordance).lower() + \" region\"\n",
    "  return temp_prompt"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:00:03.747653Z",
     "start_time": "2025-05-04T15:00:03.743789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_vertex_scores(pred_class: torch.Tensor, positive_class: int = 1):\n",
    "    \"\"\"\n",
    "    Returns vertex-wise confidence scores for the positive class.\n",
    "\n",
    "    Args:\n",
    "        pred_class (torch.Tensor): shape [N, 2], softmax logits\n",
    "        positive_class (int): which class index should be interpreted as affordance (1 or 0)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape [N], probabilities\n",
    "    \"\"\"\n",
    "    probs = F.softmax(pred_class, dim=1)\n",
    "    return probs[:, positive_class]\n",
    "\n",
    "def compute_mIoU(pred_labels: torch.Tensor, gt_labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes binary mean Intersection over Union.\n",
    "\n",
    "    Args:\n",
    "        pred_labels (torch.Tensor): shape [N], binary 0/1\n",
    "        gt_labels (torch.Tensor): shape [N], binary 0/1\n",
    "\n",
    "    Returns:\n",
    "        float: IoU score\n",
    "    \"\"\"\n",
    "    pred = pred_labels.bool()\n",
    "    gt = gt_labels.bool()\n",
    "    intersection = (pred & gt).sum().float()\n",
    "    union = (pred | gt).sum().float()\n",
    "    return (intersection / union).item() if union > 0 else float('nan')\n",
    "\n",
    "def optimize_mIoU_threshold(projected_scores, gt_labels, thresholds=None, gt_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Computes IoU over different thresholds on predicted scores.\n",
    "\n",
    "    Args:\n",
    "        projected_scores (torch.Tensor): shape [N], soft prediction per point\n",
    "        gt_labels (torch.Tensor): shape [N], soft or binary GT labels\n",
    "        thresholds (list or tensor): list of thresholds to test (default: 0.1 to 0.9)\n",
    "        gt_threshold (float): threshold to binarize ground truth\n",
    "\n",
    "    Returns:\n",
    "        (float, float): best threshold, best IoU\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = torch.linspace(0.1, 0.9, steps=9)\n",
    "\n",
    "    gt_binary = (gt_labels > gt_threshold).long()\n",
    "\n",
    "    best_iou = -1\n",
    "    best_thresh = 0.5\n",
    "\n",
    "    for t in thresholds:\n",
    "        pred_binary = (projected_scores > t).long()\n",
    "        iou = compute_mIoU(pred_binary, gt_binary)\n",
    "        print(f\"Threshold {t:.2f} → IoU: {iou:.4f}\")\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_thresh = float(t)\n",
    "\n",
    "    return best_thresh, best_iou\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:00:06.851212Z",
     "start_time": "2025-05-04T15:00:06.845749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_highlighting(sample, affordance, mlp, optimizer, render, clip_model, tokenizer, clip_transform, augment_transform, n_augs, n_iter, colors, background, output_dir, device):\n",
    "    \"\"\"\n",
    "    Optimizes the highlighting process for a given sample and affordances.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): The input sample containing mesh and semantic class information.\n",
    "        affordance: Affordance to be highlighted.\n",
    "        mlp (nn.Module): Neural network model for highlighting.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training the model.\n",
    "        render (Renderer): Renderer for generating views of the mesh.\n",
    "        clip_model (nn.Module): CLIP model for computing loss.\n",
    "        tokenizer (function): Tokenizer for text prompts.\n",
    "        clip_transform (transforms.Compose): Transformations for CLIP input images.\n",
    "        augment_transform (transforms.Compose): Augmentation transformations for images.\n",
    "        n_augs (int): Number of augmentations for CLIP loss.        n_iter (int): Number of optimization iterations.\n",
    "        colors (torch.Tensor): Tensor of colors for highlighting.\n",
    "        background (torch.Tensor): Background color tensor.\n",
    "        output_dir (str): Directory to save results.\n",
    "        device (torch.device): Device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicted class probabilities for vertices.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    export_path = os.path.join(output_dir, f\"run_{timestamp}\")\n",
    "    os.makedirs(os.path.join(export_path, \"renders\"), exist_ok=True)\n",
    "    temp_obj_path = \"data/outputDemo.obj\"\n",
    "\n",
    "    if \"semantic class\" not in sample or not isinstance(sample[\"semantic class\"], str):\n",
    "        raise ValueError(f\"Error: Missing or invalid 'semantic class' field in sample: {sample}\")\n",
    "\n",
    "    prompt = build_prompt(sample[\"semantic class\"], affordance)\n",
    "\n",
    "    points = get_coordinates(sample, device)\n",
    "    trimesh_mesh = pointcloud_to_voxel_mesh(\n",
    "        points,  # sampled point cloud from Open3D\n",
    "        resolution=16,\n",
    "        threshold=0.5,\n",
    "        export_path=temp_obj_path\n",
    "    )\n",
    "\n",
    "\n",
    "    sampled_mesh = Mesh(temp_obj_path)\n",
    "    MeshNormalizer(sampled_mesh)()\n",
    "    vertices = sampled_mesh.vertices.clone().detach().to(device).float()\n",
    "\n",
    "    losses = []\n",
    "    pred_class = None\n",
    "\n",
    "    # Optimization loop\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        optimizer.zero_grad()\n",
    "        pred_class = mlp(vertices)  # Predict highlight probabilities\n",
    "        color_mesh(pred_class, sampled_mesh, colors)  # Color mesh\n",
    "\n",
    "        # Render and compute loss\n",
    "        rendered_images, elev, azim = render.render_views(\n",
    "            sampled_mesh,\n",
    "            num_views=n_views,\n",
    "            show=False,\n",
    "            center_azim=0,\n",
    "            center_elev=0,\n",
    "            std=1,\n",
    "            return_views=True,\n",
    "            lighting=True,\n",
    "            background=background\n",
    "        )\n",
    "        loss = clip_loss(rendered_images, prompt, clip_transform, clip_model, tokenizer, device, augment_transform, n_augs)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Save the loss for logging\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Log and save intermediate results\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Last 100 CLIP score: {np.mean(losses[-100:])}\")\n",
    "            save_renders(export_path, i, rendered_images)\n",
    "            with open(os.path.join(export_path, \"training_info.txt\"), \"a\") as f:\n",
    "                f.write(f\"Iter {i}: Prompt: {prompt}, Avg CLIP score: {np.mean(losses[-100:])}, CLIP score: {loss.item()}\\n\")\n",
    "\n",
    "    # Final save and cleanup\n",
    "    save_final_results(export_path, sample[\"semantic class\"], sampled_mesh, mlp, vertices, colors, render, background)\n",
    "    with open(os.path.join(export_path, \"prompt.txt\"), \"w\") as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "    if os.path.exists(temp_obj_path):\n",
    "        os.remove(temp_obj_path)\n",
    "\n",
    "    return trimesh_mesh, pred_class, export_path"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:01:14.080013Z",
     "start_time": "2025-05-04T15:01:14.074028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utility function to validate model across validation set and log mIoUs\n",
    "\n",
    "def validate_pipeline(val_set, affordances, render_config, clip_config,\n",
    "                      transform_config, training_config,\n",
    "                      output_dir, device):\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "\n",
    "    results = []\n",
    "    print(\"\\n======= Starting validation across affordances =======\")\n",
    "\n",
    "    for sample in val_set:\n",
    "        semantic_class = sample[\"semantic class\"]\n",
    "        print(f\"\\nSample: {semantic_class}\")\n",
    "\n",
    "        valid_affordances = [a for a in get_affordance_classes(sample) if is_affordance_present(sample, a)]\n",
    "        if not valid_affordances:\n",
    "            print(\"  Skipped (no affordances present)\")\n",
    "            continue\n",
    "\n",
    "        for affordance in valid_affordances:\n",
    "            print(f\"  → Evaluating affordance: {affordance}\")\n",
    "\n",
    "            # Init model\n",
    "            mlp = NeuralHighlighter(\n",
    "                hidden_dim=training_config.get(\"hidden_dim\", 256),\n",
    "                num_layers=training_config.get(\"num_layers\", 6)\n",
    "            ).to(device)\n",
    "            optimizer = torch.optim.Adam(mlp.parameters(), lr=training_config.get(\"learning_rate\", 5e-5))\n",
    "\n",
    "            # Setup renderer\n",
    "            renderer = Renderer(dim=(render_config.get(\"res\", 224), render_config.get(\"res\", 224)))\n",
    "            n_views = render_config.get(\"n_views\", 5)\n",
    "            background = render_config.get(\"background\", torch.tensor([1., 1., 1.]).to(device))\n",
    "            colors = render_config.get(\"colors\")\n",
    "\n",
    "            # Load CLIP\n",
    "            clip_model, _ = get_clip_model(clip_config.get(\"model_name\", 'ViT-B/16'), jit=clip_config.get(\"jit\", False))\n",
    "            tokenizer = clip.tokenize\n",
    "\n",
    "            # Run pipeline\n",
    "            try:\n",
    "                mesh, pred_class, export_path = optimize_highlighting(\n",
    "                    sample=sample,\n",
    "                    affordance=affordance,\n",
    "                    mlp=mlp,\n",
    "                    optimizer=optimizer,\n",
    "                    render=renderer,\n",
    "                    clip_model=clip_model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    clip_transform=transform_config[\"clip_transform\"],\n",
    "                    augment_transform=transform_config[\"augment_transform\"],\n",
    "                    n_augs=training_config.get(\"n_augs\", 1),\n",
    "                    n_iter=training_config.get(\"n_iter\", 2000),\n",
    "                    colors=colors,\n",
    "                    background=background,\n",
    "                    output_dir=output_dir,\n",
    "                    device=device\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"  !! Failed on sample {semantic_class} / affordance {affordance}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Project and evaluate\n",
    "            pointcloud = get_coordinates(sample, device)\n",
    "            gt_labels = get_affordance_label(sample, affordance, device)\n",
    "            vertex_scores = get_vertex_scores(pred_class, positive_class=training_config.get(\"positive_class\", 0))\n",
    "            projected_scores = project_vertex_scores_to_pointcloud(mesh, vertex_scores, pointcloud)\n",
    "\n",
    "            best_thresh, best_iou = optimize_mIoU_threshold(projected_scores, gt_labels, gt_threshold=training_config.get(\"gt_threshold\", 0.0))\n",
    "\n",
    "            results.append({\n",
    "                \"class\": semantic_class,\n",
    "                \"affordance\": affordance,\n",
    "                \"iou\": best_iou,\n",
    "                \"threshold\": best_thresh,\n",
    "                \"clip_model\": clip_config.get(\"model_name\"),\n",
    "                \"lr\": training_config.get(\"learning_rate\"),\n",
    "                \"depth\": training_config.get(\"num_layers\"),\n",
    "                \"n_views\": n_views,\n",
    "                \"augmentations\": training_config.get(\"n_augs\"),\n",
    "                \"export_path\": export_path\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame for summary\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n======= Validation Summary =======\")\n",
    "    print(df.groupby(\"affordance\")[\"iou\"].mean().sort_values(ascending=False))\n",
    "\n",
    "    # Generate filename from config\n",
    "    clip_name = clip_config.get(\"model_name\", \"CLIP\").replace(\"/\", \"-\")\n",
    "    suffix = f\"clip_{clip_name}_lr{training_config.get('learning_rate')}_depth{training_config.get('num_layers')}_views{render_config.get('n_views')}_augs{training_config.get('n_augs')}\"\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(output_dir, f\"validation_results_{suffix}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved results to: {csv_path}\")\n",
    "\n",
    "    return df\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:08:43.534532Z",
     "start_time": "2025-05-04T15:03:56.575544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clip_config = {\"model_name\": \"ViT-B/16\"}\n",
    "render_config = {\"res\": 224, \"n_views\": 7, \"colors\": colors, \"background\": background}\n",
    "training_config = {\"learning_rate\": 5e-5, \"num_layers\": 6, \"n_augs\": 1, \"n_iter\": 2000, \"positive_class\": 0}\n",
    "transform_config = {\"clip_transform\": clip_transform, \"augment_transform\": augment_transform}\n",
    "\n",
    "val_df = validate_pipeline(\n",
    "    val_set, affordances,\n",
    "    render_config, clip_config, transform_config, training_config,\n",
    "    output_dir, device\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Starting validation across affordances =======\n",
      "\n",
      "Sample: Table\n",
      "  → Evaluating affordance: support\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezmiron/Desktop/Affordance_Highlighting_Project_2024/.venv/lib/python3.12/site-packages/kaolin/ops/conversions/pointcloud.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:644.)\n",
      "  vg = torch.sparse.FloatTensor(\n",
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "Unexpected type passed to requires_grad None\n",
      "Attribute \"vertex_normals\" has not been set and failed to be computed due to: 'NoneType' object has no attribute 'unsqueeze'\n",
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: ViT-B/16 on cuda (jit=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.278378963470459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 102/2000 [00:07<02:11, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28367048531770706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 202/2000 [00:13<02:06, 14.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28667845904827116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 302/2000 [00:21<02:02, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2874623739719391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 404/2000 [00:28<01:45, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2873884543776512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 504/2000 [00:34<01:36, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28825080782175067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 604/2000 [00:40<01:30, 15.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29145567715167997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 702/2000 [00:47<01:48, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.291411038339138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 802/2000 [00:54<01:22, 14.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2919712179899216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 902/2000 [01:01<01:14, 14.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29042343139648436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1002/2000 [01:09<01:42,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2909633395075798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1103/2000 [01:17<01:02, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29128217190504074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1203/2000 [01:23<00:54, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29174183040857316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1303/2000 [01:30<00:48, 14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2922312042117119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1403/2000 [01:38<00:40, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29396693021059034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1503/2000 [01:45<00:34, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2937826877832413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1603/2000 [01:52<00:27, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29369619488716125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1701/2000 [01:59<00:26, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2943024578690529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1803/2000 [02:06<00:14, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29485644489526747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1903/2000 [02:14<00:06, 14.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.29426018595695497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:21<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.10 → IoU: 0.3105\n",
      "Threshold 0.20 → IoU: 0.3105\n",
      "Threshold 0.30 → IoU: 0.2764\n",
      "Threshold 0.40 → IoU: 0.1669\n",
      "Threshold 0.50 → IoU: 0.1266\n",
      "Threshold 0.60 → IoU: 0.0990\n",
      "Threshold 0.70 → IoU: 0.0670\n",
      "Threshold 0.80 → IoU: 0.0000\n",
      "Threshold 0.90 → IoU: 0.0000\n",
      "  → Evaluating affordance: move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "Unexpected type passed to requires_grad None\n",
      "Attribute \"vertex_normals\" has not been set and failed to be computed due to: 'NoneType' object has no attribute 'unsqueeze'\n",
      "Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: ViT-B/16 on cuda (jit=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.26968467235565186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 102/2000 [00:07<02:33, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2780254051089287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 202/2000 [00:14<02:03, 14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.27987058997154235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 302/2000 [00:21<01:48, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28141076266765597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 404/2000 [00:27<01:42, 15.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2834061199426651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 502/2000 [00:34<01:48, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28718975067138675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 602/2000 [00:41<01:41, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28853362530469895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 702/2000 [00:48<01:27, 14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2896269080042839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 802/2000 [00:54<01:24, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2889507752656937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 903/2000 [01:03<01:17, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2891766920685768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1003/2000 [01:11<01:17, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2886635786294937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1103/2000 [01:18<01:01, 14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2891752487421036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1203/2000 [01:25<00:54, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2899891763925552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1303/2000 [01:32<00:47, 14.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28982201844453814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1403/2000 [01:38<00:41, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2888724768161774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1503/2000 [01:45<00:33, 14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2909116291999817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1603/2000 [01:52<00:27, 14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.28959061414003373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1703/2000 [01:59<00:20, 14.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2907213082909584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1803/2000 [02:06<00:13, 14.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2903108021616936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1903/2000 [02:13<00:06, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: -0.2905556046962738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:19<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.10 → IoU: 0.4082\n",
      "Threshold 0.20 → IoU: 0.4082\n",
      "Threshold 0.30 → IoU: 0.4082\n",
      "Threshold 0.40 → IoU: 0.0570\n",
      "Threshold 0.50 → IoU: 0.0441\n",
      "Threshold 0.60 → IoU: 0.0358\n",
      "Threshold 0.70 → IoU: 0.0189\n",
      "Threshold 0.80 → IoU: 0.0000\n",
      "Threshold 0.90 → IoU: 0.0000\n",
      "\n",
      "======= Validation Summary =======\n",
      "affordance\n",
      "move       0.408203\n",
      "support    0.310547\n",
      "Name: iou, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T14:22:08.850140Z",
     "start_time": "2025-05-04T14:22:05.413817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pointcloud = get_coordinates(val_set[0], device)\n",
    "affordance = 'support'\n",
    "gt_labels = get_affordance_label(val_set[0], affordance, device)\n",
    "visualize_affordance_pointcloud(pointcloud.cpu().numpy(), gt_labels.detach().cpu().numpy(), point_size=8.0)"
   ],
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
